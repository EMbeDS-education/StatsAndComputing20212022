{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"+4\">Programming and Data Analytics 2021/2022</font></center>\n",
    "<center><font size=\"+2\">Module 2</font></center>\n",
    "<center><font size=\"+2\">Sant'Anna School of Advanced Studies, Pisa, Italy</font></center>\n",
    "<center><img src=\"https://github.com/EMbeDS-education/StatsAndComputing20202021/raw/main/IPDP/jupyter/jupyterNotebooks/images/SSSA.png\" width=\"700\" alt=\"The extensible parallel architecture of MultiVeStA\"></center>\n",
    "\n",
    "<center><font size=\"+2\">Course responsible</font></center>\n",
    "<center><font size=\"+2\">Andrea Vandin a.vandin@santannapisa.it</font></center>\n",
    "\n",
    "<center><font size=\"+2\">Co-lecturer </font></center>\n",
    "<center><font size=\"+2\">Daniele Licari d.licari@santannapisa.it</font></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"+2\">Part 2</font></center>\n",
    "<center><font size=\"+1\">Breast Cancer Diagnosis 1</font></center>\n",
    "<center><font size=\"+1\">Overview of Data Processing</font></center>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook provides an overview of unsupervised learning pipeline**\n",
    "   * Exploratory Data Analysis (data pre-processing, missing values, visualization ...)\n",
    "   * Outliers Detection\n",
    "   * Correlation\n",
    "\n",
    "You can find more details in the [APPENDIX](#APPENDIX) of this document.\n",
    "\n",
    "In particular, this notebook will introduce the libraries:\n",
    "\n",
    "   * [scikit-learn](https://scikit-learn.org/stable/): simple and efficient tools for predictive data analysis \n",
    "   * [Seaborn](http://seaborn.pydata.org/): seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References** \n",
    "\n",
    "Some in-depth study material:\n",
    "\n",
    "* <mark> [Statistics and Machine Learning in Python, E.Duchesnay, T.Löfstedt, F.Younes](https://duchesnay.github.io/pystatsml)</mark>\n",
    "* [Topics in Statistical Learning, Francesca Chiaromonte](https://github.com/EMbeDS-education/StatsAndComputing20202021/tree/main/TSL/slides)\n",
    "* [Python for Data Analysis, 2nd edition, William Wesley McKinney (O’Reilly)](https://www.oreilly.com/library/view/python-data-science/9781491912126/)\n",
    "* [Freely available Jupyter notebooks covering the examples/material of each chapter](https://github.com/jakevdp/PythonDataScienceHandbook/tree/master/notebooks)\n",
    "* [Introduction to Data Mining (2nd Edition), Pang-Ning Tan et al.](https://www.cse.msu.edu/~ptan/)\n",
    "* [Introduction to Machine Learning Algorithms, KNIME AG](https://www.knime.com/knime-course-material-download-page)\n",
    "\n",
    "Some pictures have been taken from these sources.\n",
    "\n",
    "# What Is Machine Learning?\n",
    "\n",
    "<img src=\"images/whats_ml.jpg\" alt=\"ML\" style=\"width: 400px;\"/>\n",
    "\n",
    "Machine learning can be categorized into two main types: supervised learning and unsupervised learning.\n",
    "\n",
    "- **Supervised learning** models tries to learn the relationship between measured features X of data and some labels y associated with the data; it can be used to apply labels to new, unknown data. This is further subdivided into:\n",
    "  - *Classification* tasks, the labels are discrete categories\n",
    "  - *Regression* tasks, the labels are continuous quantities. \n",
    "\n",
    "- **Unsupervised learning** involves modeling the features of a dataset X without reference to any label, and is often described as “letting the dataset speak for itself.” These models include tasks such as:\n",
    "    - *Outlier Detection* is the identification of rare items\n",
    "    - *Dimensionality Reduction* search for a lower-dimensional representations of the data.\n",
    "    - *Clustering* identify distinct groups of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Benign and Malignant Breast Cancer Case Study \n",
    "We will analize Wisconsin Breast Cancer Dataset (WBCD), features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "<img src=\"images/Breast-Biopsy-2.jpg\" >\n",
    "\n",
    "![alt text](images/fna-benign1.png)\n",
    "![alt text](images/fna-malignant1.png)\n",
    "\n",
    "**Attribute Information**\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry \n",
    "- fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error, and \"worst\" or largest (mean of the three\n",
    "largest values) of these features were computed for each image,\n",
    "resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
    "13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "**Labels Class:**\n",
    "* malignant\n",
    "* benign\n",
    "\n",
    "\n",
    "\n",
    "This dataset is also available via the ftp server UW CS: http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/WDBC/\n",
    "\n",
    "![](images/machine_learning_cancer.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the all repository if you are on COLAB\n",
    "import os\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "course_name = 'StatsAndComputing20212022' \n",
    "\n",
    "if IN_COLAB: \n",
    "    !git clone https://github.com/EMbeDS-education/{course_name}.git\n",
    "    os.chdir(f'/content/{course_name}/PDA/jupyter/jupyterNotebooks') #for SSSA\n",
    "    # os.chdir(f'/content/{course_name}/jupyter/jupyterNotebooks')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "We will start by installing the necessary tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " # numpy for numerical computing\n",
    "%pip install numpy\n",
    "# pandas for data processing\n",
    "%pip install pandas \n",
    " # seaborn for visualization\n",
    "%pip install seaborn\n",
    "#sklearn for machine learning \n",
    "%pip install sklearn \n",
    "# scipy for statistical functions \n",
    "%pip install scipy  \n",
    "# statsmodels for statistical models\n",
    "%pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing libs\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Visualizzation libs\n",
    "# keeps the plots in one place. calls image as static pngs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import seaborn as sns # data visualization library based on matplotlib\n",
    "from IPython.display import display, Markdown # display Markdown code using Python\n",
    "\n",
    "from warnings import filterwarnings # add this only when you finished your analysis \n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset \n",
    "data = pd.read_csv('data/WBCD.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data layout: df_X and df_y\n",
    "![](images/05.02-samples-features.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Features matrix\n",
    "* Here each row of the data refers to a single observed biopsy sample.\n",
    "* By convention, this features matrix is often stored in a variable named `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features matrix \n",
    "df_X = data.iloc[:,:-1] \n",
    "# feature DataFrame\n",
    "print(df_X.shape)\n",
    "\n",
    "df_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target array\n",
    "In addition to the feature matrix X, we also generally work with a **label or target array**,\n",
    "which\n",
    "* label or target array (usually 1D) may have continuous numerical values, or discrete classes/labels\n",
    "* by convention we will usually call `y`. \n",
    "\n",
    "**Diagnosis Class:**\n",
    "* malignant\n",
    "* benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = data.iloc[:,-1] \n",
    "\n",
    "print(df_y.shape)\n",
    "df_y.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "Exploratory data analysis (EDA) is used by data scientists to analyze and investigate data sets and summarize their main characteristics. \n",
    "* It use data visualization methods to discover patterns, spot anomalies, test a hypothesis, or check assumptions.\n",
    "* It provides a better understanding of data set variables and the relationships between them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Encoding Categorical Values\n",
    "![](images/categorical_var.png)\n",
    "**Some data sets can contain categorical variables**. These variables are typically stored as text values which represent various traits.\n",
    "* Ordinal, E.g. temperature (\"high\", \"medium\", \"low\")\n",
    "* Categorical, E.g. city name (\"paris\", \"tokyo\", \"milan\").\n",
    "\n",
    "**Many machine learning algorithms can support categorical values** without further manipulation **but there are many more algorithms that do not**. \n",
    "<!-- Therefore, we should convert these text attributes into numerical values for further processing. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Find and replace* of Pandas: replace the text values with their numeric equivalent by using replace method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels with value 1 for malignant and 0 for benign\n",
    "dict_lb_to_num = {'malignant': 1, 'benign':0} # set malignat as true class\n",
    "df_y.replace(dict_lb_to_num).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(zip(df_y.unique(),range(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*sklearn.preprocessing.LabelEncoder*](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) is a utility class to transform non-numerical labels to numerical labels such that they contain only values between 0 and n_classes-1. \n",
    "This transformer should be used to encode target values, i.e. y, and not the input X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import  LabelEncoder\n",
    "# convert categorical variable (label) into numerical variable\n",
    "le = LabelEncoder()\n",
    "le.fit_transform(df_y)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical ordinal variable (feature) into numerical variable\n",
    "df_performance = pd.Series([\"good\", \"bad\", \"optimal\"], name='performance')\n",
    "le.fit_transform(df_performance)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy variable for nominal categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nominal categorical variables\n",
    "df_city= pd.DataFrame({'city':[\"paris\", \"paris\", \"tokyo\", \"milan\"]})\n",
    "df_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit_transform(df_city.city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*pandas.get_dummies*](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) convert categorical variable into dummy/indicator variables. \n",
    "* It takes only the value 0 or 1 to indicate the absence or presence of categorical values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it also called one-hot encoding which creates a binary variable (0/1) for each possible value,\n",
    "features_encoded = pd.get_dummies(df_city, columns=['city'])\n",
    "features_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "paris = (0,1,0)\n",
    "milan = (1,0,0)\n",
    "tokyo = (0,0,1)\n",
    "\n",
    "print(distance.euclidean(paris, milan))\n",
    "print(distance.euclidean(milan, tokyo))\n",
    "print(distance.euclidean(milan, paris))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<marquee style='width: 30%; color: red;'><b>Important! Golden Rules</b></marquee>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](images/categorical_var_wf.png)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style='color:black'>\n",
    "<b>Tip</b>      \n",
    "We apply get_dummies when:\n",
    "\n",
    "* The categorical (Nominal) variables are in features matrix\n",
    "\n",
    "We apply Label Encoding or Replace when:\n",
    "* The categorical variable is target array\n",
    "* The categorical (Ordinal) variables are in features matrix\n",
    "    \n",
    "</div>\n",
    "\n",
    "**Note:**\n",
    "*A categorical variable that can take exactly two values is called a binary variable (e.g. off/on). In this case, the dummy variable can be created simply by replacing the values with 0/1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "<!-- It is not unusual for an object to be missing one or more attribute values -->\n",
    "\n",
    "<!-- **Pandas treats None and NaN as essentially interchangeable for indicating missing or null values.** To facilitate this convention, there are several useful methods for detecting, removing, and replacing null values in Pandas data structures. They are:\n",
    "\n",
    "* [isnull()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html): Generate a boolean mask indicating missing values\n",
    "* [notnull()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.notnull.html): Opposite of isnull()\n",
    "* [dropna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html): Return a filtered version of the data\n",
    "* [fillna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html): Return a copy of the data with missing values filled \n",
    "\n",
    "you can find more details in the [APPENDIX](#APPENDIX). -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't null values in our dataset! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Are there null target values?   ',df_y.isnull().values.any())\n",
    "print('Are there null features values? ',df_X.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Some datasets, especially those obtained by merging multiple data sources, may contain duplicates or near duplicate instances. \n",
    " -->\n",
    "We  check for duplicate instances in the breast cancer dataset:\n",
    "* [*duplicated()*](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html): return boolean Series denoting duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = data.duplicated()\n",
    "print(f'Are there duplicate rows? = {dups.any()}')\n",
    "data = data.drop_duplicates() #  drop_duplicates return DataFrame with duplicate rows removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't duplicate instances! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "<!-- ![](images/standardization.jpg) -->\n",
    "\n",
    "Dataset can contains different magnitude, units and range. Scaling is a method used to normalize the range of independent variables or features of data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "# Why Data Scaling is important in Machine Learning\n",
    "HTML('<iframe width=\"800\" height=\"550\" src=\"https://www.youtube.com/embed/-xOUOo82kWY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Why is data scaling important?\n",
    "\n",
    "# 3 people A,B,C\n",
    "df_cm_kg = pd.DataFrame({'height_cm':[180,180,100 ], 'weight_kg':[70,69,69.1]}, index=['Dad','Mon','Son'])\n",
    "df_m_gr = pd.DataFrame({'height_m':[1.8,1.8,1.0], 'weight_gr':[70000,69000,69100]}, index=['Dad','Mon','Son'])\n",
    "\n",
    "print(df_cm_kg)\n",
    "print(df_m_gr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import draw_euclidean_distance\n",
    "\n",
    "# Draw a scatter plot DF cm and kg\n",
    "# get A, B, C\n",
    "A = df_cm_kg.iloc[0,:]\n",
    "B = df_cm_kg.iloc[1,:]\n",
    "C = df_cm_kg.iloc[2,:]\n",
    "draw_euclidean_distance(A,B,C)\n",
    "\n",
    "\n",
    "# Draw a scatter plot DF m and gr\n",
    "# get A, B, C\n",
    "A = df_m_gr.iloc[0,:]\n",
    "B = df_m_gr.iloc[1,:]\n",
    "C = df_m_gr.iloc[2,:]\n",
    "draw_euclidean_distance(A,B,C,figsize=(1,6),title='A,B,C weight and height (gr and m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is data scaling important?**\n",
    "1. Before comparing the features distribution, because differences between values of features are very high to observe on plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# create dataset Weight normal(70,10) and  Height normal(175,5)\n",
    "df_w = pd.DataFrame({'Weight': np.random.normal(70, 10, 1000)})\n",
    "df_h = pd.DataFrame({'Height': np.random.normal(175, 5, 1000)})\n",
    "\n",
    "# plot raw distributions\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "ax[0].set_title('Comparison Raw Distributions')\n",
    "sns.distplot(df_h, ax=ax[0])\n",
    "sns.distplot(df_w,ax=ax[0])\n",
    "\n",
    "# plot raw distributions\n",
    "ax[1].set_title('Comparison Standardized Distributions')\n",
    "sns.distplot(df_h.apply(lambda x: (x-x.mean())/x.std()), ax=ax[1])\n",
    "sns.distplot(df_w.apply(lambda x: (x-x.mean())/x.std()), ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Many ML algorithm calculate the distance between points by the Euclidean distance during the training phase. **The range of all features should be normalized so that each feature contributes proportionately to the final distance.**\n",
    "\n",
    "[*sklearn.preprocessing.StandardScaler*](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) standardizes the features by removing the mean and scaling to unit variance. The standard score (*z-scores*) of a sample x is calculated as:\n",
    "\n",
    "`z = (x - u) / s`\n",
    "\n",
    "where u is the mean of the data samples  and s is the standard deviation of the data samples.\n",
    "\n",
    "![](images/standardization.png) image from https://youtu.be/2tuBREK_mgE?t=165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #for Scaling the features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features =scaler.fit_transform(df_X.values)\n",
    "\n",
    "df_X_scaled = pd.DataFrame(scaled_features, index=df_X.index, columns=df_X.columns)\n",
    "df_X_scaled.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_X.apply(lambda x: (x - x.mean())/x.std())\n",
    "scaler.inverse_transform(df_X_scaled.values)\n",
    "# df_X_scaled.values * np.sqrt(scaler.var_) + scaler.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Plot\n",
    "\n",
    "Let's see the number of samples in the two classes and the percentages.\n",
    "<!-- The plot below represents the class distribution of malignant and benign samples. -->\n",
    "\n",
    "* we have 212 malignants (around 37% of the data) and 357 benign breast cancer masses (63%).\n",
    "\n",
    "Count plot visualization is done by using [seaborn.countplot](https://seaborn.pydata.org/generated/seaborn.countplot.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the counts of observations for each categorical value using bars.(a histogram for a categorical variable)\n",
    "ax = sns.countplot(df_y) # Returns the matplotlib Axes object\n",
    "\n",
    "# gets shapes and draws annotations\n",
    "for p in ax.patches: \n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2., height + 3,'{:1.2f}%'.format(height/len(df_y)*100), ha=\"center\") \n",
    "\n",
    "df_y.value_counts() # Return a Series containing counts of unique values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We plot features in 3 groups and each group includes 10 features to observe better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_mean= df_X_scaled.iloc[:,:10]\n",
    "df_X_se= df_X_scaled.iloc[10:20]\n",
    "df_X_worst =df_X_scaled.iloc[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- As the dataset contain many variables, and relationship between each and every variable is to be analysed, a pair plot is used to visualize the data further. \n",
    "\n",
    "It shows the data as a collection of points.  The position of one variable in the same data row is matched with another variable’s value. Each value is a position on either the vertical or horizontal dimension indicates its correlation.  -->\n",
    "\n",
    "### Scatterplot Matrix\n",
    "\n",
    "* It allows both, distribution of single variables and relationships between two variables.\n",
    "* It is an effective method to identify trends for analysis.\n",
    "\n",
    "To implement pair plots in python seaborn is used and is made by using [seaborn.pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html) function\n",
    "\n",
    "see https://www.python-graph-gallery.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale=1.5)\n",
    "\n",
    "#Plot pairwise relationships in a dataset.\n",
    "sns.pairplot(pd.concat([df_X_mean,df_y], axis=1), hue='target')  #hue: Grouping data points with different colors\n",
    "# sns.pairplot(pd.concat([df_X_se,df_y], axis=1),  hue='target')\n",
    "# sns.pairplot(pd.concat([df_X_worst,df_y], axis=1),  hue='target')\n",
    "# ax.set_yticklabels(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  \n",
    "\n",
    "1. The *radius*, *area* and *perimeter* characteristics are closely related.\n",
    "2. The benign tumors have smaller cell nuclei and  Binomial distribution (for *radius*, *area* and *perimeter*)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violin plot\n",
    "\n",
    "It used to represent comparison of a variable distribution (or sample distribution) across different \"categories\".\n",
    "[*seaboarn.violinplot*](https://seaborn.pydata.org/generated/seaborn.violinplot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(18,8))\n",
    "df_mean = pd.concat([df_X_mean, df_y], axis=1) \n",
    "\n",
    " \n",
    "# Unpivot the DataFrame from Wide to long format  |Target | Features| Value | [https://bit.ly/3j2Qj1O]\n",
    "data_for_violinplot = pd.melt(df_mean,id_vars=\"target\",var_name=\"features\", value_name='value')\n",
    "#violin plot\n",
    "sns.violinplot(x=\"features\", y=\"value\", hue=\"target\", data=data_for_violinplot, split=True, inner=\"quart\" )\n",
    "# sns.swarmplot(x=\"features\", y=\"value\", hue=\"target\", data=data_for_violinplot)\n",
    "plt.xticks(rotation=90) \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  \n",
    "\n",
    "1. The *mean concave points*, *mean concavity* and *mean area* have well seperated distribution on target variable (seems to be good predictors)\n",
    "2. The *mean fractal dimension*, *mean symmetry* and *mean smoothness* have similar distribution on target variable\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers Detection\n",
    "Outliers could be indicative of incorrect data, erroneous procedures or experimental areas where some theories may not be valid.\n",
    "\n",
    "*Why finding outliers is important?*\n",
    "* No Valid data (human error, erroneous procedures, instrumentation error, calibration error)\n",
    "* Data analysis and not only data cleaning (investigating the causes outliers in data)\n",
    "* Summarize data by statistics that represent the majority of the data\n",
    "* Train a model that generalizes to new data (remove outilers only in the training set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "# Why is it important to identify and understand outliers\n",
    "HTML('<iframe width=\"800\" height=\"500\" src=\"https://www.youtube.com/embed/Ua-lbOmO2NM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InterQuartile Range \n",
    "The Tukey’s boxplot is often used to pinpoint possible outliers: \n",
    "\n",
    "1. a box is drawn from the [ﬁrst quartile Q1](https://en.wikipedia.org/wiki/Quartile) to the third quartile Q3 of the data \n",
    "2. calculate the Interquartile Range  IQR = Q3 − Q1\n",
    "3. points outside the interval [Q1 − 1.5 IQR, Q3 + 1.5 IQR], called the fence (or whisker), are  marked as outliers. \n",
    "\n",
    "![](images/box_plot_ref_needed.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.boxplot( data = df_X_scaled ) \n",
    "plt.xticks(rotation=90)  \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$outlier(x)=\\begin{equation}\n",
    "\\left\\{ \n",
    "  \\begin{aligned}\n",
    "    0&   & (Q1 - 1.5 * IQR)\\leq x \\leq (Q3 + 1.5 * IQR) \\\\\n",
    "    1&   & Otherwise \n",
    "  \\end{aligned}\n",
    "  \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where X is the parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isOutlier(x , k = 1.5):\n",
    "    # Return True if x < (Q1 - 1.5 * IQR) OR  x > (Q3 + 1.5 * IQR)\n",
    "    IQR = x.quantile(0.75)-x.quantile(0.25) # Q3 - Q1 \n",
    "    minimum = x.quantile(0.25) - k * IQR    # Q1 - 1.5 IQR\n",
    "    maximum = x.quantile(0.75) + k * IQR    # Q3 + 1.5 \n",
    "    return (x < minimum) | (x > maximum )\n",
    "\n",
    "\n",
    "df_X_clean_iqr = df_X[df_X.apply(lambda x: isOutlier(x)).any(axis=1)]\n",
    "# ~ is used to invert a boolean Series, https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing\n",
    "\n",
    "# numpy.all Test whether all array elements along a given axis 1 evaluate to True. \n",
    "# we use ~isOutlier as dataframe mask to filter outliers\n",
    "# you can obtain the same result with: df_X_scaled[~df_X_scaled.apply(lambda x: isOutlier(x))].dropna()\n",
    "print(\"How many outliers?\",len(df_X_clean_iqr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:* the boxplot assumes symmetry because we add the same amount to Q3 as what we subtract from Q1.At asymmetric distributions, the usual boxplot typically ﬂags many regular data points as outlying.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score\n",
    "People often use rules to detect outliers. The classical rule is based on the **z-scores** that we introduced before.\n",
    "**This technique assumes a Gaussian distribution of the data**. The outliers are the data points that are in the tails of the distribution and therefore far from the mean. \n",
    "\n",
    "$$outlier(x)=\\begin{equation}\n",
    "\\left\\{ \n",
    "  \\begin{aligned}\n",
    "    1&   &|X-\\hat{X}| > 3 \\sigma \\\\\n",
    "    0&   &|X-\\hat{X}| \\leq 3 \\sigma \n",
    "  \\end{aligned}\n",
    "  \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where **X** is the parameter values, $\\hat{X}$ is the mean of parameter, and **σ** is its standard deviation.\n",
    "\n",
    "More precisely, the rule ﬂags xi as **outlying if |zi| exceeds 3**, say.\n",
    "(This cutoff is based on the fact that when the data are normally distributed, 99.7% of the observations fall within 3 standard deviations around the mean).\n",
    "We would have considered outliers all the absolute values plotted over the value 3 on the y axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_outiler_z = df_X_scaled[df_X_scaled.apply(lambda x: np.abs(x) > 3).any(axis=1)] \n",
    "print(\"How many outliers?\",len(df_X_outiler_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extreme outliers can be indicate no valid data\n",
    "\n",
    "#### Boxplot\n",
    "We can use boxplot simply identifying values that fall outside of 3 time the inter-quartile range (IQR) of the first or third quartile (Tukey 1977)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.boxplot( data = df_X_scaled, whis= 3.0 )\n",
    "plt.xticks(rotation=90)  \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$extremeOutlier(x)=\\begin{equation}\n",
    "\\left\\{ \n",
    "  \\begin{aligned}\n",
    "    0&   & (Q1 - 3 * IQR)\\leq x \\leq (Q3 + 3 * IQR) \\\\\n",
    "    1&   & Otherwise \n",
    "  \\end{aligned}\n",
    "  \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where X is the parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "def isExtremeOutlier(x):\n",
    "    return isOutlier(x , 3)\n",
    "\n",
    "df_X_clean_iqr_extreme = df_X[df_X.apply(lambda x: isExtremeOutlier(x)).any(axis=1)]\n",
    "print(\"How many extreme outliers?\",len(df_X_clean_iqr_extreme))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Approach\n",
    "We can use a \"visual\" approach for detection (based on density of the points above a specific threshold) and detection of the outliers (very far from the mean). \n",
    "\n",
    "We notice a not very dense region of points above the 6 value.\n",
    "    \n",
    "We suspect there are some mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.boxplot( data = df_X_scaled ) \n",
    "plt.xticks(rotation=90) \n",
    "plt.axhline(y=6, color='r', linestyle='-')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting outilers data for discussing with domino experts\n",
    "df_X_extreme_outiler=df_X_scaled[df_X_scaled.apply(lambda x: np.abs(x) > 6).any(axis=1)] \n",
    "print(\"How many extreme outliers?\",len(df_X_extreme_outiler))\n",
    "\n",
    "df_X_extreme_outiler \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving unscaled outilers data and save them into a CSV file and send it to doctors \n",
    "data_outiler_noscaled = scaler.inverse_transform(df_X_extreme_outiler)\n",
    "\n",
    "\n",
    "df_X_outiler_noscaled = pd.DataFrame(data_outiler_noscaled, index=df_X_extreme_outiler.index, columns=df_X_extreme_outiler.columns)\n",
    "df_to_check = df_X_outiler_noscaled.join(df_y)\n",
    "df_to_check.to_csv(\"data/WBCD_outliers_for_doctors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Outlier detection is important in medical applications* as well as many other applications that requires concentrating on uncommon activities. **It can identify potential patient management errors or problems with the instrumentation.**\n",
    "\n",
    "**Let's assume that the doctors confirm to us that the 12 outliers we found\n",
    "they are invalid data that we can then eliminate from our analysis** (or replace outliers with a fixed value, e.g., the median of their distribution).*\n",
    "<div class=\"alert alert-block alert-warning\" style='color:black'>\n",
    "<b>NOTE:</b>\n",
    "Before removing them we should discuss with domino experts to understand why these points are not valid (for example, the measuring equipment failed, the measurement method was unreliable for some reason, there were contaminants, etc ...).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove no valid data in original features matrix\n",
    "df_X_clean = df_X[df_X.apply(lambda x:  ((x-x.mean())/x.std()) < 6).all(axis=1)] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<marquee style='width: 30%; color: red;'><b>Important! Golden Rules</b></marquee>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**it’s important to investigate the nature of the outlier before deciding whether to drop it or not.**\n",
    "* <div class=\"alert alert-block alert-warning\" style='color:black'> <b>DROP</b> <br>If it is obvious that the outlier is due to incorrectly entered or measured data, you should drop the outlier.</div>\n",
    "\n",
    "    - For example a year field with a '9999' value. This is can be a human, instrument error or calibration error. **NOTE: if you remove invalid data you have to redo the whole analysis (e.g. scaling )**\n",
    "\n",
    "* <div class=\"alert alert-block alert-warning\" style='color:black'><b>DROP AND EXPLAIN WHY</b>\n",
    "    <br>More commonly, the outlier can affect results. In this situation, it is not legitimate to simply drop the outlier. You may run the analysis both with and without it.</div>\n",
    "   \n",
    "   - For example, you can remove the outliers (*only in the training dataset used to build the model*) and compare the result with the model obtained from raw data (with outliers). We will talk about predictive models, training sets and testing sets in the next lesson.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, removing outliers or not depends on your purpose, and from that it should become clear whether the outlier should be removed. \n",
    " - For example, if you want to create an infographic that summarizes data from a population, removing the outlier will likely have a more significant impact than keeping it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw some simple univariate outlier detection techniques,  you can find more details in the [APPENDIX](#APPENDIX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data and as a diagnostic for advanced analyses. [seaborn.heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "# Correlation and Redundant Features\n",
    "HTML('<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/O0tOmWULrf8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=0.8)\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.heatmap( df_X_clean.corr(),  square = False, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovering structure in heatmap data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can the correlation matrix as a hierarchically-clustered heatmap in order to find easly the pattern, for example the variables highly correlate with each other. [seaborn.clustermap](https://seaborn.pydata.org/generated/seaborn.clustermap.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.0)\n",
    "# Correlation is a term that is a measure of the strength of a linear relationship between two quantitative variables.\n",
    "corr_matrix = df_X_clean.corr()\n",
    "sns.clustermap(corr_matrix,annot=True,fmt=\".2f\",figsize=(20,14)) # Plot the correlation matrix as a hierarchically-clustered heatmap.\n",
    "plt.title(\"Correlation Between Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  \n",
    " Radius, area and perimeter (mean, the worst, and the standard error ) have stronge positive correlation. \n",
    "</div>\n",
    "\n",
    "[seaborn.jointplot](http://seaborn.pydata.org/generated/seaborn.jointplot.html) : plotting a bivariate relationship and distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats #  library of statistical functions (we'll use to compute pearson correlation)\n",
    "\n",
    "# Plot correlation between 2 features and distribution\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# seaborn.jointplot draws a plot of two variables with bivariate and univariate graphs\n",
    "ax = sns.jointplot(df_X_clean.loc[:,'mean area'], \n",
    "              df_X_clean.loc[:,'mean radius'], \n",
    "              kind=\"reg\" )\n",
    "\n",
    "ax.ax_joint.text(-1,5, f\"Pearsonr's correlation: {stats.pearsonr(df_X_clean.loc[:,'mean area'], df_X_clean.loc[:,'mean radius'])[0]:.2f}\")\n",
    "\n",
    "# sns.jointplot(df_X_clean.loc[:,'mean radius'], \n",
    "#               df_X_clean.loc[:,'mean area'], \n",
    "#               kind=\"reg\")\n",
    "# sns.jointplot(df_X_clean.loc[:,'mean area'], \n",
    "#               df_X_clean.loc[:,'mean perimeter'], \n",
    "#               kind=\"reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing multi colinearity\n",
    "<div class=\"alert alert-block alert-warning\" style='color:black'>\n",
    "Two highly correlated input variables probably carry similar information. \n",
    "We can remove multi colinearity, it means the columns are dependenig on each other so we should avoid it because what is the use of using same column twice.\n",
    "</div>\n",
    "\n",
    "So, we can:\n",
    "- drop _worst radius_,_worst area_,_worst perimeter_, _mean perimeter_, _mean radius_ columns (they have a strong correlation with  _mean area_) and leave only _mean area_.\n",
    "- drpo _area error_, _perimeter error_ and  leave only _area error_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop multicolinearity variables\n",
    "df_X_clean = df_X_clean.drop(['worst radius','worst area','worst perimeter','perimeter error', 'radius error', 'mean perimeter', 'mean radius'],axis=1)\n",
    "df_X_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the preprocessed data on CSV file\n",
    "df_X_y_clean = df_X_clean.join(df_y)\n",
    "df_X_y_clean.to_csv(\"data/WBCD_preprocessed.csv\")  # create the 'data' folder before to save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation ‘among the predictors’ is a problem to be rectified to be able to come up with a reliable model.\n",
    "Another measure that is commonly used to help diagnose multicollinearity is presented in the [APPENDIX](#APPENDIX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appendix includes further information on the course topics but **will not be exam topics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[3, 2 ],[3, np.nan],[4,4]], columns=list('AB'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dropna()* function is used to remove missing values. Determine if rows or columns which contain missing values are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Boolean mask\\n{df.isnull()} \\nDrop missing values')\n",
    "df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns with too many missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null = pd.DataFrame([[3, 2 ],[3, np.nan],[4,np.nan]], columns=list('AB'))\n",
    "df_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_null.dropna(axis=1)\n",
    "# Drop columns using that limit\n",
    "limit = len(df_null) * 0.7 # How many non-NA values in each column/row? 2\n",
    "df_null.dropna(axis=1, thresh=limit) # columns having null values more than or equal to 70 percent are dropped from the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, the missing values in the 'B' column are replaced by the median value of that column. The values before and after replacement are shown for a subset of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['B'] =  df.B.fillna(df.B.median()) # median() exclude NA/null values when computing the result by default\n",
    "print(f\"Replace missing value in 'B' column by the median value \\n{df}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style='color:black'>\n",
    "<b>Tip fillna</b>      \n",
    "We replace :\n",
    "\n",
    "* The mean  of the numerical column data is used to replace null values when the data is normally distributed\n",
    "* Median is used if the data comprised of outliers.\n",
    "* Mode is used when the data having more occurences of a particular value or more frequent value.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_city= pd.DataFrame({'city':[\"paris\", \"paris\", \"\", \"milan\"]})\n",
    "# df_city = df_city.replace({\"\": np.nan}) \n",
    "# df_city.info()\n",
    "# most_freq_city = df_city['city'].mode().iloc[0]\n",
    "# most_freq_city\n",
    "# df_city.city.fillna(most_freq_city) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### KNNImputer\n",
    "[KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) The k nearest neighbors algorithm can be used for imputing missing data by finding the k closest neighbors to the observation with missing data and then imputing them based on the the non-missing values in the neighbors.\n",
    "\n",
    "Each sample’s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "X = [[1, 2, np.nan], \n",
    "     [3, 4, 3], \n",
    "     [2, 6, 5], \n",
    "     [8, 8, 7]]\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Convert Skewed data to Normal Distribution\n",
    "When it comes to skewed distributions, the most common\n",
    "response is to transform the data\n",
    "Generally, the most common type of skewness is\n",
    "right-skewness\n",
    "Consequently, the most common type of transformation is the\n",
    "log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skewnorm\n",
    "df_skewed = pd.DataFrame({'car_crash': skewnorm.rvs(10, size=100)+1}) # positive skewed data\n",
    "sns.distplot(df_skewed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "## log  transformation: can't be applied to zero or negative values\n",
    "sns.distplot(np.log(df_skewed))\n",
    "##square root transformation\n",
    "sns.distplot(np.sqrt(df_skewed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploratory data analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\"\"\"\n",
    " if you obtain Error executing Jupyter command 'lab': [WinError 5] Accesso negato \n",
    "    With JupyterLab closed, right click on JupyterLab and click \"Run As Administrator\"\n",
    "    Then, when you launch this command it will work.\n",
    "\"\"\"    \n",
    " # mito for  Excel-like interface\n",
    "!{sys.executable} -m pip install mitoinstaller\n",
    "!{sys.executable} -m mitoinstaller install \n",
    "\n",
    "\n",
    "\n",
    "print(\"\"\"   \n",
    "NOTE: After installation \n",
    "Please shut down the currently running JupyterLab and relaunch it to enable Mito    \n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mitosheet\n",
    "mitosheet.sheet??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Interactive graphs\n",
    "The [plotly](https://plotly.com/python/) Python library is an interactive, open-source plotting library that supports over 40 unique chart types.\n",
    "\n",
    "Built on top of the Plotly JavaScript library (plotly.js), plotly enables Python users to create beautiful interactive web-based visualizations that can be displayed in Jupyter notebooks, saved to standalone HTML files, or served as part of pure Python-built web applications using Dash.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly for  web-based visualizations\n",
    "%pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotly Express is the easy-to-use, high-level interface to Plotly (https://plotly.com/python/plotly-express/)\n",
    "import plotly.express as px\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "fig = px.histogram(df_y, x='target', color=\"target\", \n",
    "                   color_discrete_map={ # replaces default color mapping by value (ref https://i.stack.imgur.com/lFZum.png)\n",
    "                    \"malignant\": \"tomato\", \"benign\": \"limegreen\"}\n",
    "            )\n",
    "HTML(fig.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_X_mean,df_y], axis=1)\n",
    "\n",
    "fig = px.scatter_matrix(df, color='target',height=1000, title='Scatter Matrix'\n",
    "                       , color_discrete_map={ # replaces default color mapping by value (ref https://i.stack.imgur.com/lFZum.png)\n",
    "                \"malignant\": \"tomato\", \"benign\": \"limegreen\"\n",
    "            })\n",
    "\n",
    "fig.update_layout(\n",
    "    font_size=9, # set fontsize = 9\n",
    "    plot_bgcolor= 'white',# set background color = white\n",
    ")\n",
    "\n",
    "\n",
    "HTML(fig.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#violin plot\n",
    "fig = px.violin(data_for_violinplot,x=\"features\", y=\"value\",  color=\"target\", box=True,\n",
    "                title='Violin Matrix',\n",
    "                color_discrete_map={ # replaces default color mapping by value (ref https://i.stack.imgur.com/lFZum.png)\n",
    "                \"malignant\": \"tomato\", \"benign\": \"limegreen\"\n",
    "            })\n",
    "\n",
    "HTML(fig.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(data_for_violinplot,x=\"features\", y=\"value\",  color=\"target\", \n",
    "             color_discrete_map={ # replaces default color mapping by value (ref https://i.stack.imgur.com/lFZum.png)\n",
    "                \"malignant\": \"tomato\", \"benign\": \"limegreen\"\n",
    "            })\n",
    "\n",
    "HTML(fig.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(df_X_mean.corr(), width=800, color_continuous_scale='RdBu_r')\n",
    "HTML(fig.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving into html page\n",
    "with open('interactive_chart.html', 'w') as f:\n",
    "    f.write(fig.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.python-graph-gallery.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring dataset: Decision Tree\n",
    "Although still under data exploration, we can get useful information by training a Decision Tree from all characteristics. We will explain the Decision Tree model better in the second part of this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X = df_X.values\n",
    "y = df_y.values \n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=8)  \n",
    "clf.fit(X, y)\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "out = tree.plot_tree(clf, \n",
    "                   feature_names=df_X.columns,  \n",
    "                   class_names=['Malignant','Benign'],\n",
    "                   filled=True)\n",
    "for o in out: \n",
    "    arrow = o.arrow_patch\n",
    "    if arrow is not None:\n",
    "        arrow.set_edgecolor('red')\n",
    "        arrow.set_linewidth(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Leys C, Delacre M, Mora YL, Lakens D, Ley C. How to Classify, Detect, and Manage Univariate and Multivariate Outliers, With Emphasis on Pre-Registration. International Review of Social Psychology. 2019;32(1):5. DOI: http://doi.org/10.5334/irsp.289](https://www.rips-irsp.com/articles/10.5334/irsp.289). \n",
    "2. [Rousseeuw, P.J. and Hubert, M. (2018), Anomaly detection by robust statistics. WIREs Data Mining Knowl Discov, 8: e1236.](https://doi.org/10.1002/widm.1236)\n",
    "\n",
    "They suggest the use of the median absolute deviation (MAD) to detect univariate outliers. \n",
    "Again, the idea is conceptually similar to computed z-scores: for each value, subtract the median from it, and divide by the median of the absolute deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad(x):\n",
    "    return 1.483 * np.median(np.abs(x- np.median(x)))\n",
    "\n",
    "df_X_clean_mad = df_X[df_X.apply(lambda x: (x - np.median(x))/mad(x) <= 3.0).all(axis=1)]\n",
    "print(\"How many outliers?\",len(df_X)-len(df_X_clean_mad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate\n",
    "[Sklearn includes different advanced outlier detection methods based on machine learning (ML)](https://scikit-learn.org/stable/modules/outlier_detection.html) can handle correlated multivariate dataset, detect abnormalities within them, and do not assume a normal distributions of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LocalOutlierFactor\n",
    "LocalOutlierFactor (LOF) algorithm computes a score (called local outlier factor) reflecting the degree of abnormality of the observations. \n",
    "It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "X = df_X.values\n",
    "\n",
    "# The LOF score of an observation is equal to the ratio of the average local density of his k-nearest neighbors, and its own local density:\n",
    "# a normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density.\n",
    "clf = LocalOutlierFactor(n_neighbors=20)\n",
    "clf.fit_predict(X)\n",
    "# The higher, the more normal. Inliers tend to have a LOF score close to 1 (negative_outlier_factor_ close to -1), while outliers tend to have a larger LOF score.\n",
    "lof = clf.negative_outlier_factor_ \n",
    "\n",
    "# we obtained the threshold value from the scores by using the quantile function.\n",
    "# Here, we got the lowest 2 percent of score values as the anomalies.\n",
    "thresh = np.quantile(lof, .02)\n",
    "\n",
    "# We extracted the anomalies by comparing the threshold value and identify the values of elements.\n",
    "index = np.where(lof<=thresh)\n",
    "values = X[index]\n",
    "len(values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation: VIF \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another measure that is commonly used to help diagnose multicollinearity is the **variance inflation factor (VIF)**.\n",
    "Although correlation matrix and scatter plots can also be used to find multicollinearity, their findings only show the bivariate relationship between the independent variables. **VIF is preferred as it can show the correlation of a variable with a group of other variables.**\n",
    "\n",
    "\n",
    "VIF measures how much of the variation in one variable is explained by the other variable. This is done by running a regression using one of the correlated x variables as the dependent variable against the other variables as predictor variables.\n",
    "![](images/vip.png)\n",
    "\n",
    "Use the following guidelines to interpret the VIF:\n",
    "\n",
    "|VIF| Status of predictors|\n",
    "|------|------|\n",
    "|VIF = 1|Not correlated|\n",
    "|1 < VIF < 5|Moderately correlated|\n",
    "|VIF > 5|Highly correlated|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "# VIF dataframe \n",
    "vif_data = pd.DataFrame() \n",
    "vif_data[\"feature\"] = df_X_scaled.columns\n",
    "\n",
    "# calculating VIF for each feature \n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(df_X_scaled.values, i) \n",
    "                          for i in range(len(df_X_scaled.columns))] \n",
    "  \n",
    "print(vif_data.sort_values(by='VIF', ascending=False ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _mean radius_ is  highly correled to the other variable. **Dropping variables should be an iterative process starting with the variable having the largest VIF.**\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
