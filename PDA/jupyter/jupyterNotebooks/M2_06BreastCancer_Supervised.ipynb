{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"+4\">Programming and Data Analytics 2021/2022</font></center>\n",
    "<center><font size=\"+2\">Module 2</font></center>\n",
    "<center><font size=\"+2\">Sant'Anna School of Advanced Studies, Pisa, Italy</font></center>\n",
    "<center><img src=\"https://github.com/EMbeDS-education/StatsAndComputing20202021/raw/main/IPDP/jupyter/jupyterNotebooks/images/SSSA.png\" width=\"700\" alt=\"The extensible parallel architecture of MultiVeStA\"></center>\n",
    "\n",
    "<center><font size=\"+2\">Course responsible</font></center>\n",
    "<center><font size=\"+2\">Andrea Vandin a.vandin@santannapisa.it</font></center>\n",
    "\n",
    "<center><font size=\"+2\">Co-lecturer </font></center>\n",
    "<center><font size=\"+2\">Daniele Licari d.licari@santannapisa.it</font></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"+2\">Part 2</font></center>\n",
    "<center><font size=\"+1\">Breast Cancer Diagnosis 3</font></center>\n",
    "<center><font size=\"+1\">Supervised learning: Classification</font></center>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook provides an overview of supervised learning pipeline**\n",
    "\n",
    "   * use Scikit-learn library package for classification task\n",
    "   * understand metrics of classification performance evaluation\n",
    "   * demonstrate the problem of model overfitting.\n",
    "   * tune the hyperparameters and model validation\n",
    "\n",
    "This notebook presents Python programming examples for machine learning supervised: splitting data, predictive models, decision boundary, evaluation, cross-validation, hyperparameters tuning.  \n",
    "You can find more details in the [APPENDIX](#APPENDIX) of this document.\n",
    "\n",
    "**References** \n",
    "This notebook is mainly based on the following books:\n",
    "\n",
    "* <mark>[Statistics and Machine Learning in Python, E.Duchesnay, T.Löfstedt, F.Younes](ftp://ftp.cea.fr/pub/unati/people/educhesnay/pystatml/StatisticsMachineLearningPythonDraft.pdf)</mark>\n",
    "* [Topics in Statistical Learning, Francesca Chiaromonte](https://github.com/EMbeDS-education/StatsAndComputing20202021/tree/main/TSL/slides)\n",
    "* [Python for Data Analysis, 2nd edition, William Wesley McKinney (O’Reilly)](https://www.oreilly.com/library/view/python-data-science/9781491912126/)\n",
    "* [Freely available Jupyter notebooks covering the examples/material of each chapter](https://github.com/jakevdp/PythonDataScienceHandbook/tree/master/notebooks)\n",
    "* [Introduction to Data Mining (2nd Edition), Pang-Ning Tan et al.](https://www.cse.msu.edu/~ptan/)\n",
    "* [Introduction to Machine Learning Algorithms, KNIME AG](https://www.knime.com/knime-course-material-download-page)\n",
    "\n",
    "Some pictures have been taken from these sources. \n",
    "\n",
    "Read the step-by-step instructions below carefully. To execute the code, click on the corresponding cell and press the SHIFT-ENTER keys simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Visual Introduction to Decision Tree\n",
    "Supervised learning models tries to learn the relationship between measured features X of data and some labels y associated with the data. This is further subdivided into:\n",
    "  - *Classification* tasks, the labels are discrete categories\n",
    "  - *Regression* tasks, the labels are continuous quantities. \n",
    "\n",
    "The following interactive example was written by Stephanie and Tony on R2D3. we will create a machine learning model to distinguish homes in New York from homes in San Francisco. \n",
    "\n",
    "It introduces you to the decision boundary, decision tree, test model, and overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML,IFrame \n",
    "IFrame(src=\"http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\", width=1424, height=650) # doesn't work on colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Gini impurity measure\n",
    "\n",
    "![](images/gini.png) image from https://youtu.be/u4IxOk2ijSs?t=65\n",
    "\n",
    "More details how to calculate the gini index for the decsion tree: https://youtu.be/7VeUPuFGJHk?t=428"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Benign and Malignant Breast Cancer \n",
    "We will first take a look at a simple classification task, features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "<img src=\"images/Breast-Biopsy-2.jpg\" >\n",
    "\n",
    "**Attribute Information**\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry \n",
    "- fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error, and \"worst\" or largest (mean of the three\n",
    "largest values) of these features were computed for each image,\n",
    "resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
    "13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "**Classification Task**\n",
    "\n",
    "We want predict the benign or malignant breast cancer using machine learning \n",
    "\n",
    "**Diagnosis Class:**\n",
    "* malignant\n",
    "* benign\n",
    "\n",
    "![alt text](images/fna-benign1.png)\n",
    "![alt text](images/fna-malignant1.png)\n",
    "\n",
    "This dataset is also available via the ftp server UW CS: http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/WDBC/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the all repository if you are on COLAB\n",
    "import os\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "course_name = 'StatsAndComputing20212022' \n",
    "\n",
    "if IN_COLAB: \n",
    "    !git clone https://github.com/EMbeDS-education/{course_name}.git\n",
    "    os.chdir(f'/content/{course_name}/PDA/jupyter/jupyterNotebooks') #for SSSA\n",
    "    # os.chdir(f'/content/{course_name}/jupyter/jupyterNotebooks')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing libs\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Visualizzation libs\n",
    "# keeps the plots in one place. calls image as static pngs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "from IPython.display import display, Markdown # display Markdown code using Python\n",
    "import seaborn as sns # data visualization library based on matplotlib\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Let's load preprocessed data by the Part 1 into Pandas, and get *Features matrix* and *Target array*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset, we will first load the data into a Pandas DataFrame object and display its content\n",
    "\n",
    "data = pd.read_csv('data/WBCD_preprocessed.csv', index_col=0)\n",
    "df_X = data.iloc[:,:-1] # Features matrix\n",
    "df_y = data.iloc[:,-1] # Target array\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels with value 1 for malignant and 0 for benign\n",
    "dict_lb_to_num = {'malignant':1, 'benign':0} # set malignat as true class\n",
    "df_y = df_y.replace(dict_lb_to_num)\n",
    "df_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data\n",
    "**We would like to evaluate the model on data it has not seen before**, and so we will split the data into a training set and a testing set. \n",
    "1. *Training dataset*: Dataset used to fit the model (set the model parameters like weights).\n",
    "2. *Test dataset (holdout set)*: Dataset (hold-out) used to provide an unbiased evaluation of a final model fit on the training dataset. \n",
    "\n",
    "we hold back a subset of the data from dataset, and then use this holdout set to check the model performance. \n",
    "\n",
    "![](images/splitting_1.png)\n",
    "\n",
    "**We usually split the data around 70%-30% (or 80%-20%) between training and testing sets.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<marquee style='width: 30%; color: red;'><b>Important! Golden Rules</b></marquee>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Never use testing data for fitting an estimator\n",
    "- Use testing data only for evaluation (please don't touch the testing set)\n",
    "\n",
    "<!-- <img src='images/training_2.jpg' width='500px'> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style='color:black'>\n",
    "    \n",
    "Some classification problems can present a large imbalance in the distribution of target classes. <b>It is important to have the same data distribution across test and training set.</b>\n",
    "In such cases it is recommended to use <b>stratified sampling</b> to ensure that relative class frequencies is approximately on both sets.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[*sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html):\n",
    "Split arrays or matrices into random train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data is splitted 70%-30% in a stratified method on the class labels. \n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(df_X, df_y, test_size =.3, stratify=df_y, random_state=1) \n",
    "\n",
    "print(f\"N. Training sample {len(y_train)}, {len(y_train)/len(df_y)*100:.2f}%\")\n",
    "print(f\"N. Testing sample {len(y_test)}, {len(y_test)/len(df_y)*100:.2f}%\")\n",
    "\n",
    "# plot target distributions of test and training set\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "fig = plt.figure(figsize = (10,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "ax = sns.countplot(y_train.sort_values())   \n",
    "ax.set_title(\"Training Set\")\n",
    "plt.subplot(1, 2, 2)\n",
    "ax = sns.countplot(y_test.sort_values()) \n",
    "ax.set_title(\"Test Set\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "we use the StandardScaler:\n",
    "* *StandardScaler.fit_transform()* on train set to learn the parameters of scaling (means and standard deviations) on the train data and scale data.\n",
    "* *StandardScaler.transform()* on test set to scale data based on the  scaling paramaters learned on the train data \n",
    "\n",
    "**Our models do not have to rely on parameters learned from the test set**\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style='color:black'>\n",
    "Including the test dataset in the transform computation will allow information to flow from the test data to the train data and therefore to the model that learns from it, thus allowing the model to cheat (introducing a bias). </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Standard Scaler\n",
    "\n",
    "# # fit on training\n",
    "x_train_mean = X_train_raw.mean()\n",
    "x_train_std = X_train_raw.std()\n",
    "\n",
    "# # transform training\n",
    "(X_train_raw-x_train_mean)/x_train_std\n",
    "\n",
    "# # transform testing\n",
    "(X_test_raw-x_train_mean)/x_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #for Scaling the features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw) # computes std and mean of each feature and scaling. \n",
    "X_test  = scaler.transform(X_test_raw)  # scales the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "The basic recipe for applying a supervised machine learning model:\n",
    "1. Choose a class of model\n",
    "\n",
    " <del>2. Choose best model hyperparameters </del>(we will see later)\n",
    " \n",
    "3. Fit the model to the training data\n",
    "4. Use the model to predict labels for new data\n",
    "\n",
    "[Sklearn contains several supervised machine learning models](https://scikit-learn.org/stable/supervised_learning.html).\n",
    "We use a Decision Tree Classifier which predicts the value of a target variable by learning simple decision rules (if-then-else) inferred from the data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 1. instantiate model\n",
    "model = DecisionTreeClassifier(random_state=42) \n",
    "# 2. fit model to data (training)\n",
    "model.fit(X_train, y_train)   \n",
    "# 3. predict on new data \n",
    "y_pred = model.predict(X_test)             \n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 1. instantiate model\n",
    "model2 = KNeighborsClassifier() \n",
    "# 2. fit model to data (training)\n",
    "model2.fit(X_train, y_train)   \n",
    "# 3. predict on new data \n",
    "y_pred2 = model.predict(X_test)             \n",
    "y_pred2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics of classification performance evaluation\n",
    "Let's draw the decision boundary of Decision Tree Classifier trained on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.utils import draw_boundary\n",
    "\n",
    "# # draw decision boundary for visualization (and teaching) purpose\n",
    "# draw_boundary(DecisionTreeClassifier(random_state = 42),  X_train, X_test, y_train, y_test,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  \n",
    "Figure above shows what happened when we added new data to the model (Testing set). It turned out that the model did not fit well with the new data. Notice that the model misclassified some of the new data.\n",
    "</div>\n",
    "\n",
    "![](images/confusion_matrix_sick.png)\n",
    "image from [Machine Learning: Testing and Error Metrics](https://www.youtube.com/watch?v=aDW44NPhNw0)\n",
    "\n",
    "We can use the following metrics:\n",
    "- *True Positive (TP)*  : Malignant cancer correctly identified as Malignant\n",
    "- *True Negative (TN)*  : Benign cancer correctly identified as Benign\n",
    "- *False Positive (FP)* : Benign cancer incorrectly identified as  Malignant\n",
    "- *False Negative (FN)* : Malignant cancer incorrectly identified as Benign\n",
    "\n",
    "\n",
    "\n",
    "The four outcomes can be formulated in a 2×2 confusion matrix\n",
    "\n",
    "![](images/confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Accuracy (ACC)*: (TP + TN) / (TP + FP + FN + TN)\n",
    "* *Sensitivity (SEN)* or Recall of the positive class or true positive rate (TPR) or hit rate: SEN = TP / P = TP / (TP+FN)\n",
    "* *Specificity (SPC)* or Recall of the negative class or true negative rate: SPC = TN / N = TN / (TN+FP)\n",
    "* *Precision* or positive predictive value (PPV): PPV = TP / (TP + FP)\n",
    "\n",
    "<!-- ![](https://miro.medium.com/max/700/1*kaqtNALKZujx1FGlbK11OQ.png) -->\n",
    "In addition, *F1 Score* (or F-score) which is the harmonic mean of the precision and recall is usefull to deal with imbalaced datasets.\n",
    "* *F1 = 2 * (precision * recall) / (precision + recall)*\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style='color:black'>\n",
    "<b>NOTE:</b>\n",
    "Intuitively, **precision** (on positive class) is the ability of the classifier **not to label as positive a sample that is negative**, and **recall** (on positive class) is the ability of the classifier to **find all the positive samples** (TPR).The F1 score penalizes the model more when Accuracy or Recall is low.</div>\n",
    "\n",
    "![](images/evaluation.png)\n",
    "image from [Machine Learning: Testing and Error Metrics](https://www.youtube.com/watch?v=aDW44NPhNw0)\n",
    "\n",
    "![](images/credit_card_fraud.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "plot_confusion_matrix(model, X_test, y_test) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a text report showing the main classification metrics.\n",
    "print('='*20,'Training Set Results','='*20)\n",
    "print(classification_report(y_train, model.predict(X_train)))\n",
    "\n",
    "print('='*20,'Testing Set Results','='*20)\n",
    "report_testing_dtree = classification_report(y_test, y_pred)\n",
    "print(report_testing_dtree)\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) Text summary of the precision, recall, F1 score for each class. \n",
    "The support is the number of occurrences of each class in y_train or y_test.\n",
    "The reported averages include macro average (averaging the unweighted mean per label) and weighted average (averaging the support-weighted mean per label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style='color:black'>\n",
    "\n",
    "\n",
    "<b>In our case, the value of Recall of positive class (SEN) is considered a primary value, as the goal is to identify all cases of really positive (malignant tumor) minimizing false negative (e.g. I predict benign tumor but the patient has a malignant tumor)</b>. The malignant '1' tumors have positive class and those benign '0' negative class.\n",
    "False positive (Malignant cancer incorrectly identified as Benign) can be identified by further investigation by doctors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.utils import plot_decision_tree\n",
    "# plot_decision_tree(model, X_train, y_train, df_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. instantiate model\n",
    "# model = DecisionTreeClassifier(random_state=42) \n",
    "\n",
    "# # 2. remove outliers\n",
    "# X_train_no_outliers = X_train[(np.abs(X_train)<3).all(axis=1)]\n",
    "# y_train_no_outliers = y_train[(np.abs(X_train)<3).all(axis=1)]\n",
    "# print(\"How many outliers?\",len(X_train_raw)-len(X_train_no_outliers))\n",
    "\n",
    "# # 3. fit model to data (training)\n",
    "# model.fit(X_train_no_outliers, y_train_no_outliers)   \n",
    "# # 4. predict on new data \n",
    "# y_pred = model.predict(X_test) \n",
    "\n",
    "# # 5. evaluation\n",
    "# print('='*5,'Testing Set Results (No Outliers in Training)','='*5)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`The anecdote of the Wisdom of the crowd.` At a 1906 country fair in Plymouth, 800 people participated in a contest to estimate the weight of an ox. Anybody won the contest, but the statistician Francis Galton observed that the median guess, 1207 pounds, was accurate within 1% of the true weight of 1198 pounds.\n",
    "\n",
    "---\n",
    "\n",
    "In ensemble learning theory, we combine several weak learners (or base models) models in order to create a strong learner (or ensemble model) that achieves better performances (generalize well from the training data).\n",
    "\n",
    "[Random forests or random decision forests](https://scikit-learn.org/stable/modules/ensemble.html#forest) are an ensemble learning method for classification (or regression) that operate **by constructing a multitude of random decision trees** (decision trees are created using a subset of the input feature) at training time and **outputting the class that is the mode of the classes** (classification) or mean prediction (regression) of the individual trees. \n",
    " \n",
    "![](images/Random_forest_diagram_complete.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. instantiate model Random Forest with 50 decision trees\n",
    "model = RandomForestClassifier(n_estimators=50,random_state=42) \n",
    "# 2. fit model to data\n",
    "model.fit(X_train, y_train)   \n",
    "# 3. predict on new data\n",
    "y_pred = model.predict(X_test)    \n",
    "\n",
    "# # drawing decision boundary  (only for teaching)\n",
    "# draw_boundary(RandomForestClassifier(n_estimators=50,random_state=42) ,  X_train,X_test,y_train,y_test )\n",
    "\n",
    "# 4. Evaluation (Comparison with decision tree model)\n",
    "print('='*20,'Testing DECISION TREE Results','='*20)\n",
    "print(report_testing_dtree)\n",
    "\n",
    "print('='*20,'Testing RANDOM FOREST Results','='*20)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In Random Forest, multiple overfitting estimators can be combined to reduce the overfitting effect and find a better classification**\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style='color:black'>\n",
    "<b>NOTE:</b>\n",
    "Since our model does not have huge data set, testing set is left with few observations to lead any real conclusion.\n",
    "    </div>\n",
    "    \n",
    "<!--  <div class=\"alert alert-block alert-info\" style='color:black'> -->\n",
    "\n",
    "<!-- One disadvantage of using a holdout set for model validation is that we have lost a portion of our data to the model training. In the previous case, the 30% of the dataset does not contribute to the training of the model! This is not optimal, and can cause problems especially if the initial set of training data is small. -->\n",
    "One way to address this is to use <a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)\" >Cross-Validation</a>\n",
    "<!-- </div> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "**Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It combines (averages) measures of performace in prediction on different iterations to obtain a more accurate estimate of model prediction performance.**\n",
    "\n",
    "K-fold Cross-Validation scheme randomly divides the set of observations into 𝐾 groups, or folds, of approximately equal size and perform the analysis on one subset ( K-1 folds, called the training set), and validating the analysis on the other subset (1 fold, called the validation set or testing set)\n",
    "To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.\n",
    "\n",
    "Visually, it might look something like this:\n",
    "\n",
    "<img src='images/K-fold_cross_validation.png' >\n",
    "\n",
    " <div class=\"alert alert-block alert-info\" style='color:black'>\n",
    "<b>Tip</b>\n",
    "When you have a small dataset, it recommended to use cross-validation scheme to estimate the global predictive power of a learning algorithm. \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [*StratifiedKFold()*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html?highlight=stratifiedkfold#sklearn.model_selection.StratifiedKFold) which is a variation of k-fold which returns stratified folds. \n",
    "\n",
    "<img src='images/stratifiedKFold_2.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "def cv_scores_explained(model, X, y):\n",
    "    # Stratified 5 folds \n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # Lists to store scores by folds \n",
    "    recalls_train, recalls_test, f1_test, acc_test = list(), list(), list(), list()\n",
    "    \n",
    "    # for each round fitting model on K-1 folds and test it on 1 fold        \n",
    "    for train_index, test_index in cv.split(X, y): # split() generate indices to split data into training and test set.\n",
    "        \n",
    "        ## 1. getting data for each round ##\n",
    "        X_train = X[train_index, :]  # features data for fitting model        \n",
    "        X_test  = X[test_index, :]   # features data for testing model\n",
    "        y_train = y[train_index]     # target vector for fitting model    \n",
    "        y_test  = y[test_index]      # target vector for testing model\n",
    "        \n",
    "               \n",
    "        ## 2. data scaling ##  \n",
    "        scaler = StandardScaler()\n",
    "        # fitting and scaling a StandardScaler\n",
    "        X_train = scaler.fit_transform(X_train) # learn the parameters and scaling  \n",
    "        # scaling on test set\n",
    "        X_test = scaler.transform(X_test) # scaling using training parameters\n",
    "        \n",
    "        ## 3. fit model ##\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        ## 4. prediciton on test set ##\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        ## 5. evalution model ##\n",
    "        # getting testing accuracy, f1 and recall score \n",
    "        acc = metrics.accuracy_score(y_test, y_pred)\n",
    "        f1 = metrics.f1_score(y_test, y_pred)\n",
    "        recall = metrics.recall_score(y_test, y_pred)        \n",
    "        # getting training recall score \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        recall_training = metrics.recall_score(y_train, y_train_pred)\n",
    "        \n",
    "        ## 6. saving partial 'fold' results ##\n",
    "        recalls_test.append(recall)\n",
    "        acc_test.append(acc)\n",
    "        f1_test.append(f1)\n",
    "        recalls_train.append(recall_training)\n",
    "        \n",
    "    return pd.DataFrame({'train_recall':recalls_train, 'test_recall':recalls_test,'test_f1':f1_test,'test_accuracy':acc_test}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_X.values  # Features matrix\n",
    "y = df_y.values # target array\n",
    "estimator = DecisionTreeClassifier(random_state=42)\n",
    "cv_scores_explained(estimator,X,y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) function:\n",
    "* allows specifying multiple metrics for evaluation.\n",
    "* returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "# A Pipeline makes it easier to compose estimators, providing this behavior under cross-validation:\n",
    "from sklearn.pipeline import make_pipeline\n",
    "    \n",
    "def cv_scores(model, xvalues, yvalues):\n",
    "    \"\"\"\n",
    "    Evaluate metric(s) by cross-validation \n",
    "    Return CV scores and scores of the model for each run of the cross validation.\n",
    "    \"\"\"\n",
    "\n",
    "    # for the complete metrics:  https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    scoring=('recall','precision','accuracy', 'f1') # accuracy,recall, precision, f1 on positive class \n",
    "    \n",
    "    # Also in cross-validation, data transformations should be learned from a training set and applied to held-out data for prediction \n",
    "    clf = make_pipeline(StandardScaler(), model)\n",
    "    \n",
    "    # cross_validate uses Stratified 5 folds (80%-20% splitting data) https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html\n",
    "    cv_results = cross_validate(clf, xvalues, yvalues, cv=5, return_train_score=True, scoring=scoring)\n",
    "    \n",
    "    # Folds scores\n",
    "    df_folds = pd.DataFrame(cv_results)        \n",
    "    df_cv_scores = df_folds[sorted(cv_results.keys())].mean() # CV scores\n",
    "    df_cv_scores['model'] = type(model).__name__\n",
    "    \n",
    "    return  df_cv_scores, df_folds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_X.values  # Features matrix\n",
    "y = df_y.values # target array\n",
    "estimator = DecisionTreeClassifier(random_state=42)\n",
    "df_dtree_cv, df_folds = cv_scores(estimator,X,y)\n",
    "# df_dtree_cv\n",
    "df_folds.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree model is an over fit. There are big differences between training scores and testing scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier(n_estimators=50,random_state=42)\n",
    "df_rf_cv, _  = cv_scores(estimator,X,y)\n",
    "df_rf_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scores comparison\n",
    "df_cv_scores = pd.DataFrame([df_dtree_cv,df_rf_cv])\n",
    "df_cv_scores[['model','test_accuracy','test_f1','test_precision','test_recall']].style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier mitigates the overfitting problem reducing the difference between training and testing scores, but we can do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Go to pump our model!\n",
    "![](images/model_tuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is caused by making a model more complex than necessary.\n",
    "\n",
    "![](images/overfitting.jpg)\n",
    "\n",
    "The Model's ability to generalize to new data is based on factors such as:\n",
    "- the complexity of the model\n",
    "- the model's performance on training data respect on test set\n",
    "\n",
    "We must find the trade-off between fitting our data well, but also fitting the data as simply as possible. \n",
    "\n",
    "![](images/validation.png)\n",
    "\n",
    " \n",
    "<!-- In first article of Stephanie and Tony on R2D3, we created a model that distinguishes homes in San Francisco from those in New York. In the [second article](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/), they will talk about tuning and the Bias-Variance tradeoff. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we saw the basic recipe for applying a supervised machine learning model. Now we can complete the recipe by adding Hyperparameters tuning to deal with the overfitting problem:\n",
    "1. Choose a class of model\n",
    "2. **Choose best model hyperparameters**\n",
    "3. Fit the model to the training data\n",
    "4. Use the model to predict labels for new data\n",
    "\n",
    "In machine learning, a **hyperparameter is a parameter whose value is used to control the learning process and complexity of the model**. \n",
    "\n",
    "<!-- The choice of model and choice of hyperparameters are the most important part of using these tools and techniques effectively. -->\n",
    "In order to make an informed choice, we need a way to validate that our model and our hyperparameters are a good fit to the data.\n",
    "To do this, **we need to retain some subset of training data (validation set) to evaluate the performance of the model to changes in its hyperparameters.**\n",
    "\n",
    "<!-- ![](img/splitting_validation.png) -->\n",
    "<img src='images/splitting_validation.png' width='800px'>\n",
    "\n",
    "As we have seen before, one disadvantage of using a holdout set for model validation is that we have lost a portion of our data to the model training. This is not optimal, and can cause problems especially if the initial set of training data is small.  Thus, to overcome this, cross validation of 5 fold is performed to validate the result.\n",
    "\n",
    "<!-- ![](notebook/img/splitting_2.png) -->\n",
    "<img src='images/splitting_2.png' width='700px'>\n",
    "<!-- **Why do we need a validation set?**\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. \n",
    "\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision-tree learners can create over-complex trees that do not generalise the data well**. This is called overfitting. \n",
    "\n",
    "Mechanisms such as pruning, **setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.**\n",
    "[Pruning](https://en.wikipedia.org/wiki/Decision_tree_pruning) reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.\n",
    "\n",
    "![](images/pruning.png)\n",
    "\n",
    "<img src='images/cp.png' width='620px'>\n",
    "<!-- --Bias is the difference between the Predicted Value and the Expected Value\n",
    "--The Variance is when the model takes into account the fluctuations in the data i.e. the noise as well. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [*min_samples_leaf*](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) tunable parameter to tune the decision tree model complexity and [sklearn.model_selection.validation_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html#sklearn.model_selection.validation_curve) to plot the influence of a single hyperparameter on the training score and the validation score in order to find out whether the estimator is overfitting or underfitting for some hyperparameter values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import plot_decision_tree\n",
    "dt = DecisionTreeClassifier(random_state = 42,min_samples_leaf=161)\n",
    "dt.fit(X_train,y_train)\n",
    "plot_decision_tree(dt , X_train,y_train, df_X.columns)\n",
    "# evalutation\n",
    "# test_score = metrics.recall_score(y_test, dt.predict(X_test))\n",
    "# train_score = metrics.recall_score(y_train, dt.predict(X_train))\n",
    "# print(f'Train score {train_score:.2f}, Test score {test_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import validation_curve_model\n",
    "dtree =  DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "param_range = range(1,34,4) \n",
    "print(list(param_range))\n",
    "param_name=\"min_samples_leaf\"\n",
    "ylim=[0.50, 1.01]\n",
    "\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "validation_curve_model(X_train,y_train, dtree, param_name, param_range, cv=5, ylim=ylim )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above  is validation curve, and we see the following essential features:\n",
    "* The training score is everywhere higher than the validation score. This is generally the case: the model will be a better fit to data it has seen than to data it has not seen.\n",
    "* **For very high model complexity (a high-variance model, min_samples_leaf value from 1 to 13), the training data is overfit**, which means that the model predicts the training data very well, but fails for any previously unseen data.\n",
    "* **For very low model complexity (a high-bias model, min_samples_leaf value after 20), the training data is underfit**, which means that the model is a poor predictor both for the training data and for any previously unseen data.\n",
    "* For some intermediate value, **the validation curve has a maximum. This level of complexity indicates a suitable trade-off between bias and variance is 17**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    " In practice, models generally have more than one knob to turn, and thus plots of\n",
    "validation  curves change from lines to multidimensional surfaces. \n",
    "\n",
    "Grid Search tries all the possible combinations of parameter values (knobs to turn) which are evaluated using CV and the best combination is retained.\n",
    "\n",
    "\n",
    "<img src='images/grid_search.png' width='400' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print({\"min_samples_leaf\": list(range(1,30,2)), # len = 15\n",
    "        \"min_samples_split\":list(range(2,8,1)), # len = 6\n",
    "        \"max_depth\": list(range(1,20,2)) # len = 10\n",
    "       })\n",
    "# all possibile combinations = 15 * 10 * 6 = 900 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*sklearn.model_selection.GridSearchCV*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html): Exhaustive search over specified parameter values for an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree =  DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "parameters_grid={\"min_samples_leaf\": range(1,30,2),\n",
    "                 \"min_samples_split\":range(2,8,1),\n",
    "                 \"max_depth\": range(1,20,2)\n",
    "               }\n",
    " \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#(Stratified)5Fold,\n",
    "clf = GridSearchCV(dtree, parameters_grid, scoring='recall',n_jobs=4, verbose=2)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Recall score ', clf.best_score_, ' with ', clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42, max_depth=3, min_samples_leaf=17, min_samples_split=3)\n",
    "dt.fit(X_train,y_train)\n",
    "plot_decision_tree(dt , X_train,y_train, df_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalution on Test Set\n",
    "dtree = DecisionTreeClassifier(random_state=42, max_depth=3, min_samples_leaf=17, min_samples_split=3)\n",
    "dtree.fit(X_train, y_train)    \n",
    "print('='*20,'Testing Fine-Tuned DecisionTree Results','='*20)\n",
    "print(classification_report(y_test, dtree.predict(X_test) ))\n",
    "\n",
    "# Evalution using Cross Validation\n",
    "print('='*20,'Cross Validation Fine-Tuned DecisionTree ','='*20)\n",
    "df_tuned_dtree_cv, _ = cv_scores(dtree,X,y)\n",
    "df_tuned_dtree_cv['model'] = 'Fine-Tuned DecisionTree'\n",
    "df_tuned_dtree_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf =  RandomForestClassifier(random_state = 42)\n",
    "\n",
    "parameters_grid={\"min_samples_leaf\": range(1,10,2),\n",
    "                 \"min_samples_split\":range(2,6,1),\n",
    "                 \"max_depth\": range(1,10,2),\n",
    "                 \"n_estimators\": range(20,101,20)\n",
    "               }\n",
    " \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GridSearchCV(rf, parameters_grid,scoring='recall',n_jobs=4,verbose=2)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Recall score ', clf.best_score_, ' with ', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalution on Test Set\n",
    "model = RandomForestClassifier(random_state=42, max_depth=7, min_samples_leaf=1, min_samples_split=3,n_estimators=20) \n",
    "model.fit(X_train, y_train)    \n",
    "print('='*20,'Testing Fine-Tuned RandomForest Results','='*20)\n",
    "print(classification_report(y_test, model.predict(X_test) ))\n",
    "\n",
    "# Cross Validation\n",
    "print('='*20,'Cross Validation Fine-Tuned RandomForest ','='*20)\n",
    "df_tuned_rf_cv, _ = cv_scores(model,X,y)\n",
    "df_tuned_rf_cv['model'] = 'Fine-Tuned RandomForest'\n",
    "df_tuned_rf_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_scores = pd.DataFrame([df_dtree_cv,df_tuned_dtree_cv, df_rf_cv,df_tuned_rf_cv]).sort_values('test_recall', ascending=False)\n",
    "df_cv_scores[['model','test_accuracy','test_f1','test_precision','test_recall',]].style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuned RandomForest models has 92.7% recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Models\n",
    "Pickle is the standard way of serializing objects in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_path = 'data'\n",
    "# save models\n",
    "pickle.dump(scaler, open(f\"{model_path}/scaler\", 'wb'))\n",
    "pickle.dump(model, open(f\"{model_path}/best_trained_model\", 'wb')) \n",
    "\n",
    "# load models\n",
    "loaded_scaler = pickle.load(open(f\"{model_path}/scaler\", 'rb')) \n",
    "loaded_model = pickle.load(open(f\"{model_path}/best_trained_model\", 'rb')) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleX = df_X.sample(2)\n",
    "make_pipeline(loaded_scaler,loaded_model).predict(sampleX) # Apply transforms to the data, and predict with the final estimator\n",
    "# loaded_model.predict(loaded_scaler.transform(sampleX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "Below is a flowchart of typical workflow in model training. The best parameters can be determined by grid search techniques.\n",
    "\n",
    "<img src='images/grid_search_workflow.png' width='400' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer # to load the dataset\n",
    "from sklearn.preprocessing import StandardScaler #for Scaling the features\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from IPython.display import display, Markdown \n",
    "import pickle\n",
    "\n",
    "class MLPipeline():\n",
    "    \n",
    "    def __init__(self,model,test_size=0.3,k_folds=5):\n",
    "        self.model = model\n",
    "        self.model_name = type(model).__name__\n",
    "        self.test_size = test_size\n",
    "        self.k_folds = k_folds\n",
    "        self.scaler = None\n",
    "\n",
    "    def load_data(self):\n",
    "        #load dataset \n",
    "        self.X,self.y = load_breast_cancer(return_X_y=True)         \n",
    "\n",
    "\n",
    "    def split_data(self):\n",
    "        # Splitting train and test data \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X,self.y, test_size=self.test_size, stratify=self.y, random_state=1)\n",
    "        \n",
    "    def preprocessing(self):\n",
    "        # Scaling features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train =self.scaler.fit_transform(self.X_train)              \n",
    "        self.X_test =self.scaler.transform(self.X_test) \n",
    "        \n",
    "    def parameter_tuning(self, parameters):\n",
    "        # GridSearchCV to find optimal parameters\n",
    "        grid = GridSearchCV(self.model, parameters, cv=self.k_folds)\n",
    "        grid.fit(self.X_train, self.y_train)\n",
    "\n",
    "        # Getting optimal parameters\n",
    "        self.best_score = grid.best_score_   \n",
    "        self.best_params= grid.best_params_\n",
    "        self.best_model = grid.best_estimator_        \n",
    "        \n",
    "\n",
    "    def model_evaluation(self):\n",
    "        # Getting train and test accuracy        \n",
    "        self.best_model.fit(self.X_train, self.y_train)\n",
    "        self.train_accuracy = self.best_model.score(self.X_train, self.y_train)\n",
    "        self.test_accuracy = self.best_model.score(self.X_test, self.y_test)\n",
    "        \n",
    "    def k_fold_cross_validation(self):\n",
    "        # Model definition\n",
    "        if self.scaler:\n",
    "            pipeline = make_pipeline(StandardScaler(),self.best_model)\n",
    "        else:\n",
    "            pipeline = self.best_model\n",
    "        # Applying k-fold and getting scores\n",
    "        self.kfold_scores = cross_val_score(pipeline, self.X, self.y, cv=self.k_folds)\n",
    "        \n",
    "    def save_model(self,model_path):\n",
    "        # Save optimum model and Scaler\n",
    "        if self.scaler:\n",
    "            pickle.dump(self.scaler,  open(f\"{model_path}/StandardScaler\", 'wb'))\n",
    "        pickle.dump(self.tree, open(f\"{model_path}/best_model\", 'wb'))        \n",
    "\n",
    "    def markdown_report(self):\n",
    "        display(Markdown(\n",
    "        f\"\"\"---\n",
    "        \\n Model: {self.model_name}\n",
    "        \\nDataset size {len(self.X)}, Test size {self.test_size*100:.1f}% \n",
    "        \\n*Optimal parameters (GridSearchCV {self.k_folds}-Folds ):*\n",
    "        \\n   - Best params **{self.best_params}**\n",
    "        \\n   - Accuracy **{self.best_score:.2f}** \n",
    "        \\n*Model_evaluation (test size {self.test_size*100:.1f}%):*\n",
    "        \\n   -  Train accuracy **{self.train_accuracy:.2f}**\n",
    "        \\n - Test accuracy **{self.test_accuracy:.2f}**\n",
    "        \\n*Cross Validation Score :*\n",
    "        \\n   - Avarage Accuracy **{np.mean(self.kfold_scores):.2f}**   +/-{np.std(self.kfold_scores):.2f} \\n---\"\"\" ))\n",
    "   \n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=1)\n",
    "parameters_grid = {'criterion': ['gini','entropy'], 'splitter': ['best','random'], 'max_depth': [2,3,4]}\n",
    "\n",
    "pipeline = MLPipeline(model=model)\n",
    "pipeline.load_data()\n",
    "pipeline.split_data()\n",
    "pipeline.preprocessing()\n",
    "pipeline.parameter_tuning(parameters_grid)\n",
    "pipeline.model_evaluation()\n",
    "pipeline.k_fold_cross_validation()\n",
    "# pipeline.save_model('data')\n",
    "pipeline.markdown_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style='color:black'>\n",
    "    <b>NOTE:</b>\n",
    "In an ideal world, you should have an independent test set to verify the performance of your model. \n",
    "Sometimes it happens that our dataset is not large enough to be divided into training, validation, and testing sets, so people use cross-validation to use as much data as possible for both, training and testing.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Imbalanced Datasets\n",
    "Imbalanced datasets are those where there is a severe skew in the class distribution.This bias in the training dataset can influence many machine learning algorithms, leading some to ignore the minority class entirely. \n",
    "\n",
    "There are two major groups of selection algorithms:\n",
    "\n",
    "- the under-sampling method by randomly selecting a given number of samples from the majority class.\n",
    "- the over-sampling methods by repeating some samples (or use some heuristic) and balance the number of samples between the dataset\n",
    "\n",
    "Below, how to use sampling techniques implemented in the [imbalanced-learn Python library](https://imbalanced-learn.org/stable/) to deal with imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install  install imbalanced-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under-sampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "sampler = RandomUnderSampler(random_state=0)\n",
    "X_res_u, y_res_u = sampler.fit_resample(X_train, y_train)   \n",
    "pd.Series(y_res_u).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  over-sampling\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "sampler = RandomOverSampler(random_state=0)\n",
    "X_res_os, y_res_os = sampler.fit_resample(X_train, y_train)   \n",
    "pd.Series(y_res_os).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  over-sampling using a heuristic\n",
    "#instead repeating the same samples when over-sampling, we can use some specific heuristic instead like SMOTE to create synthetic data\n",
    "sampler = SMOTE(random_state=0) # Synthetic Minority Oversampling Technique https://arxiv.org/pdf/1106.1813.pdf\n",
    "X_res_smote, y_res_smote = sampler.fit_resample(X_train, y_train)   \n",
    "pd.Series(y_res_smote).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imb_dataset = {'RandomUnderSampler':(X_res_u, y_res_u),\n",
    "              'RandomOverSampler':(X_res_os, y_res_os),\n",
    "               'SMOTE':(X_res_smote, y_res_smote)}\n",
    "for n, d in imb_dataset.items():\n",
    "    model.fit(d[0], d[1])  #fit model to data (training)\n",
    "    y_pred = model.predict(X_test) #predict on new data \n",
    "\n",
    "    # evaluation\n",
    "    print('='*10,f'Testing Set Results ({n})','='*10)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features importance   \n",
    "\n",
    "The importance of a feature is computed as the (normalized) total\n",
    "reduction of the criterion brought by that feature.\n",
    "It is also known as the Gini importance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many model forms describe the underlying impact of features relative to each other. In scikit-learn, Decision Tree models and ensembles of trees such as Random Forest, Gradient Boosting, and Ada Boost provide a feature_importances_ attribute when fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=42) \n",
    "# 2. fit model to data (training)\n",
    "model.fit(X_train, y_train)   \n",
    "\n",
    "pd.Series(dict(zip(df_X.columns,model.feature_importances_ ))).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values).  \n",
    "    \n",
    ":func:`sklearn.inspection.permutation_importance` as an alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sklearn.inspection.permutation_importance](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html)\n",
    "First, a baseline metric, defined by scoring, is evaluated on a (potentially different) dataset defined by the X. Next, a feature column from the validation set is permuted and the metric is evaluated again. The permutation importance is defined to be the difference between the baseline metric and metric from permutating the feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload \n",
    "# %autoreload 2\n",
    "from src.utils import get_importance_features\n",
    "get_importance_features(model, X_test, y_test, df_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML,IFrame \n",
    "IFrame(src=\"http://www.r2d3.us/visual-intro-to-machine-learning-part-2/\", width=1200, height=650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Predictive Models\n",
    "\n",
    "### Logistic regression\n",
    "Logistic regression is called a generalized linear models. ie.: it is a linear model with a link\n",
    "function that maps the output of linear multiple regression to the posterior probability of class\n",
    "1 𝑝(1|𝑥) using the logistic sigmoid function: 𝑝(1|𝑤, 𝑥𝑖) = 1/1 + exp(−𝑤 · 𝑥𝑖)\n",
    "\n",
    "### K-Nearest Neighbor\n",
    "In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small).\" [ wiki](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "\n",
    "### Support Vector - linear\n",
    "It simply finds a line or curve (in two dimensions) or manifold (in multiple dimensions) that divides the classes from each other. A linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification. [wiki](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    " \n",
    "### Naive Bayes\n",
    "It relies on Bayes's theorem. In Bayesian classification, we're interested in finding the probability of a label given some observed features, which we can write as $P(L~|~{\\rm features})$. \n",
    " Such a model is called a generative model because it specifies the hypothetical random process that generates the data. In Gaussian Naive Bayes classifier, the assumption is that data from each label is drawn from a simple Gaussian distribution. Gaussian distribution is used as prior probability distribution to compute the posterior probability  $P(L~|~{\\rm features})$\n",
    " \n",
    "### Gradient Boosting\n",
    "Gradient boosting is an ensemble algorithm that fits boosted decision trees by minimizing an error gradient.\n",
    "Decision trees can be added together (sequentially) to correct for errors in the predictions. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier.\n",
    "\n",
    "### Neural Network\n",
    "A MultiLayer Perceptron (MLP) is a class of feedforward artificial neural network. An MLP consists of at least three layers of nodes. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# define a dictory for classifier and their paramaters range\n",
    "dict_classifiers = {\n",
    "    \"Logistic Regression\": \n",
    "            {'classifier': LogisticRegression(random_state=42,max_iter=1000),\n",
    "                'params' : [\n",
    "                            {\n",
    "                             'penalty': ['l1','l2'],\n",
    "                             'C': [0.01,0.1,1,10,100],\n",
    "                                \n",
    "                            }\n",
    "                           ]\n",
    "            },\n",
    "    \"Nearest Neighbors\": \n",
    "            {'classifier': KNeighborsClassifier(),\n",
    "                 'params': [\n",
    "                            {\n",
    "                            'n_neighbors': [1, 3, 5, 10],\n",
    "                            'leaf_size': [3, 30]\n",
    "                            }\n",
    "                           ]\n",
    "            },\n",
    "             \n",
    "    \"Linear SVM\": \n",
    "            {'classifier': SVC(random_state=42),\n",
    "                 'params': [\n",
    "                            {\n",
    "                             'C': [1, 10, 100],\n",
    "                             'gamma': [0.001, 0.0001],\n",
    "                             'kernel': ['linear']\n",
    "                            }\n",
    "                           ]\n",
    "            },\n",
    "    \"Gradient Boosting\": \n",
    "            {'classifier': GradientBoostingClassifier(random_state=42),\n",
    "                 'params': [\n",
    "                            {\n",
    "                             'learning_rate': [0.05, 0.1],\n",
    "                             'n_estimators' :[50, 100, 200],\n",
    "                             'max_depth':[3,7]\n",
    "                            }\n",
    "                           ]\n",
    "            },\n",
    "    \"Decision Tree\":\n",
    "            {'classifier': DecisionTreeClassifier(random_state=42),\n",
    "                 'params': [\n",
    "                            {\n",
    "                             'min_samples_leaf':[17], \n",
    "                             'min_samples_split': [2],\n",
    "                             'max_depth':[3,7]\n",
    "                            }\n",
    "                             ]\n",
    "            },\n",
    "    \"Random Forest\": \n",
    "            {'classifier': RandomForestClassifier(random_state=42),\n",
    "                 'params': {\n",
    "                            'n_estimators' :[20,50],\n",
    "                            'max_depth':[3,7],\n",
    "                            'min_samples_leaf': [1], \n",
    "                            'min_samples_split': [3]\n",
    "                           }\n",
    "            },\n",
    "    \"Naive Bayes\": \n",
    "            {'classifier': GaussianNB(),\n",
    "                 'params': {}\n",
    "            },\n",
    "    \"Neural Network\":\n",
    "            {'classifier': MLPClassifier(random_state=42),\n",
    "                 'params': {\n",
    "                     'hidden_layer_sizes': [(100,),(50,50), (20,10,10,10)],\n",
    "                     'max_iter':[600,1000]\n",
    "                 }\n",
    "            }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def evalution_classifiers(dict_classifiers, X,y,scaler=True, verbose = False ):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =.3, stratify=y, random_state=10) \n",
    "    model_scores = {}\n",
    "    \n",
    "    if scaler:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train) # computes variance and mean of each feature and scaling. \n",
    "        X_test  = scaler.transform(X_test)  # scales the test data.\n",
    "    \n",
    "    for clf_name, classifier in dict_classifiers.items():\n",
    "        print(f\"= Training {clf_name} =\")\n",
    "        \n",
    "        # computing GridSearchCV for search the best parameters\n",
    "        grid = GridSearchCV(classifier['classifier'], \n",
    "                      classifier['params'],\n",
    "                      scoring='recall',n_jobs=4, refit=True) \n",
    "        grid.fit(X_train,y_train)\n",
    "        \n",
    "        # using the best model (with best parameters) for evalaution\n",
    "        df_report = pd.DataFrame(classification_report(y_true = y_test, y_pred = grid.best_estimator_.predict(X_test), output_dict=True))\n",
    "        model_scores[clf_name] = df_report\n",
    "        \n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(f\" - Best params {grid.best_params_}\")\n",
    "            print(df_report)\n",
    "\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "X,y = load_breast_cancer(return_X_y=True) \n",
    "df_cv_scores = evalution_classifiers(dict_classifiers, X,y)\n",
    "recall_scores = [(k, v.iloc[1,1]) for k,v in df_cv_scores.items()]\n",
    "print(\"RECALL\")\n",
    "recall_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "### Decision Tree Regressor\n",
    "The process of solving regression problem with decision tree (DecisionTreeRegressor class) using Scikit Learn is very similar to that of classification. \n",
    "\n",
    "Differences to Decision Trees:\n",
    "* Splitting criterion: minimizing error (|True values – Predicted values|)\n",
    "* Leaf node predicts average target values of training instances reaching that node \n",
    "\n",
    "The evaluation metrics for regression differ from those of classification. The rest of the process is almost same.\n",
    "\n",
    "Metrics:\n",
    "* Mean Absolute Error (MAE) = 1/n Σ|True values – Predicted values|\n",
    "* Mean Squared Error (MSE) = 1/n Σ(True values – Predicted values)**2\n",
    "* Root Mean Squared Error (RMSE) = sqrt(MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "print(load_diabetes().DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# X, y = load_diabetes(return_X_y=True)\n",
    "df = load_diabetes(as_frame=True)\n",
    "X = df['data']\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "regressor.fit(X_train,y_train)\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor\n",
    "It's similar to RandomForestClassifier, but it combines DecisionTreeRegressor and outputs the average of the individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "regressor = RandomForestRegressor(random_state=0)\n",
    "regressor.fit(X_train,y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Lesson\n",
    "Please follow the instructions **[HERE](https://github.com/EMbeDS-education/SNS-IProML2021/wiki/Setup-your-machine#install-knime-analytics-platform)** to install *KNIME Analytics Platform*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
