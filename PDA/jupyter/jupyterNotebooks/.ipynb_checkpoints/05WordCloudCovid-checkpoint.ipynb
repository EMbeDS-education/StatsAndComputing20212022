{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"+4\">Programming and Data Analytics 1 2021/2022</font></center>\n",
    "<center><font size=\"+2\">Sant'Anna School of Advanced Studies, Pisa, Italy</font></center>\n",
    "<center><img src=\"https://github.com/EMbeDS-education/StatsAndComputing20212022/raw/main/PDA/jupyter/jupyterNotebooks/images/SSSA.png\" width=\"700\" alt=\"EMbeDS\"></center>\n",
    "\n",
    "<center><font size=\"+2\">Course responsible</font></center>\n",
    "<center><font size=\"+2\">Andrea Vandin a.vandin@santannapisa.it</font></center>\n",
    "\n",
    "<center><font size=\"+2\">Co-lecturer </font></center>\n",
    "<center><font size=\"+2\">Daniele Licari d.licari@santannapisa.it</font></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrkbsmwRhuWe"
   },
   "source": [
    "<center><font size=\"+4\">Lecture 5: </font></center>\n",
    "<center><font size=\"+2\"> Creation of word clouds for COVID-related online news</font></center>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isBU8c9fhuWg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xkGN1b3huWg"
   },
   "source": [
    "# Computing and visulatizing the most important words in online news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXy-eVmEhuWh"
   },
   "source": [
    "This example shows that functions can hide a lot of complexity.\n",
    "* E.g., we can download remote data by just invoking a function\n",
    "* In particular, in this example we will:\n",
    "    1.\tDownload all the articles the online service [NewsAPI](https://newsapi.org/)\n",
    "    2.\tCombine the articles into one document (`str`)\n",
    "    3.\tClean data\n",
    "    4.\tCompute word-frequency pairs\n",
    "    5.\tVisualize the analysis in a Word Cloud\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Some definitions:*\n",
    "\n",
    "A **web service** is an application run by a web server that can be called from a series of URLs that will return their data in a format intended for parsing by a \"generic\" computer program rather than by a browser. As a result, web services can use something like XML (especially SOAP or so) or JSON is used.\n",
    "\n",
    "An **Application Programming Interface (API)** allows two systems to communicate with one another. An API exactly defines the methods for one software program to interact with the other. \n",
    "\n",
    "![](images/web_server_web_service.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9lHWyf1huWj"
   },
   "source": [
    "This document has been __created by Daniele Licari__ and elaborated by Andrea Vandin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxDJuJC9huWk"
   },
   "source": [
    "## The modules we need to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onPRcvz2huWl",
    "outputId": "32669025-bfbc-4a6c-e869-e46b8a9f8956",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install newsapi-python\n",
    "%pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Z_n5daChuWm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# integrating News API into your Python application \n",
    "from newsapi import NewsApiClient \n",
    "\n",
    "# creating wordclouds into your Python application \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#  In Python, string.punctuation will give the all sets of punctuation. \n",
    "import string\n",
    "\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJv2ycUJhuWo"
   },
   "source": [
    "__NewsApiClient__ is a local client for the online service [__NewsAPI__](https://newsapi.org/) that allows you to get news from [eveywhere in the world](https://newsapi.org/sources)\n",
    "* Intuitively, an online service is like an online function running in a remote server (computer)\n",
    "* A local client is a piece of code runnning in your machine that communicates with the online service\n",
    "  * It allows you to easily interact with the online service\n",
    "  * The interaction is typically done through a set of message exchanges as prescribed by the APIs of the service\n",
    "* We need a `api_key` that is a univoque identifier \n",
    "  * necessary when making requests to be identified\n",
    "  * can be obtained registering [here](https://newsapi.org/register)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyscgnVIYpFA"
   },
   "source": [
    "There exist many online services accessible with Python\n",
    "* Most of them follow this api-key approach\n",
    "* E.g. to enforce subscriptions\n",
    "<!-- * In our case, __we are using a free subscription that allows us to only get the first 250 characters from a news item__ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6XRBgRzhuWq"
   },
   "source": [
    "## Implementing two functions for data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPRRvof_huWq"
   },
   "source": [
    "__Stop words__ are usually the most common words in any text ( like “the”, “of”, “to” and “and”), \n",
    "* They don’t tell us much about the actual content in a text\n",
    "* These are the words we want to ignore - this is our data cleaning\n",
    "\n",
    "We want to find the words that will help us differentiate a text from texts that are about different subjects. \n",
    "* __We will filter out the common words__.\n",
    "* Of course, we are going to use a function for doing this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhqbA-ZdhuWr"
   },
   "source": [
    "Furthermore, we want to ignore punctuation \n",
    "* In Python, string.punctuation will give the all sets of punctuation.\n",
    "* We are going to use it to delete all punctuation from a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjYmoOxBhuWs"
   },
   "outputs": [],
   "source": [
    "# to remove STOPWORDS from a list of strings\n",
    "def remove_stopwords(wordlist):\n",
    "    my_stopwords = {'usa','today','verge','bbc','cnn'}\n",
    "    return [w for w in wordlist if (w not in STOPWORDS) and (w not in my_stopwords)]\n",
    "\n",
    "# to remove Punctuations from a string\n",
    "def remove_punctuations(my_str): \n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char  not in string.punctuation:\n",
    "            no_punct = no_punct + char\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNPcmrzohuWt"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlUhhhK9huWt"
   },
   "source": [
    "We first get the most relevant articles (top 100) about coronavirus, published in usa-today\n",
    "* This looks like a normal function invocation\n",
    "* But under the hood a lot happens to contact the remote service \n",
    "(https://newsapi.org/v2/everything?q=coronavirus&sources=usa-today&language=en&pageSize=100&sortBy=relevancy&apiKey=345f8a0aa8c64d549fde1d8343d036f8)\n",
    "\n",
    "Luckily, the function `get_everything` hides this complexity to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Whc4M8nhuWu"
   },
   "outputs": [],
   "source": [
    "newsapi = NewsApiClient(api_key='345f8a0aa8c64d549fde1d8343d036f8')\n",
    "json_data = newsapi.get_everything(q='coronavirus',        # All the articles that contain this word\n",
    "                                    language='en',\n",
    "                                    sources = 'usa-today', # The newspaper of interest\n",
    "                                    #sources = 'cnn', # The newspaper of interest\n",
    "                                    #sources = 'bbc-news', # The newspaper of interest\n",
    "                                    #sources = 'bbc-news,the-verge'\n",
    "                                    #\n",
    "                                    page_size=100,           # Get 100 articles\n",
    "                                    sort_by='relevancy',\n",
    "                                    \n",
    "                                   \n",
    "                                    )\n",
    "#There exist many more parameters allowing us the ask for more articles, different sources, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwuXEhVShuWv"
   },
   "source": [
    "The return value `json_data` is just a dictionary collecting information about the remote request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9lBSMEehuWv",
    "outputId": "691d24ea-c776-4a24-fcad-85552f9a2988"
   },
   "outputs": [],
   "source": [
    "print(type(json_data))\n",
    "print()\n",
    "print(json_data.keys())\n",
    "print()\n",
    "print('The status of the request is',json_data['status'],'There are ',json_data['totalResults'],'articles matching the request.')\n",
    "print('We got',len(json_data['articles']),'articles')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMSGhhuqhuWy"
   },
   "source": [
    "We can ignore all the metadata (i.e. the data about the remote request) and focus on the actual articles only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGJecHG2huWy",
    "outputId": "023d913b-9d6a-40e0-ccd4-d6e4c4487157"
   },
   "outputs": [],
   "source": [
    "articles = json_data['articles'] \n",
    "\n",
    "#Get the description of the first article\n",
    "content = articles[0]['description']\n",
    "print('The content:')\n",
    "print(content)\n",
    "print('The content is long',len(content),\"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWzx75OtYpFL"
   },
   "source": [
    "Now let's combine the content of all the news into variable `contents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-qaKrK_huWz",
    "outputId": "c961b973-6427-4ee1-ec2c-975ec2fe6706"
   },
   "outputs": [],
   "source": [
    "def combine_articles(list_of_articles):\n",
    "    contents = ''\n",
    "    for article in list_of_articles:\n",
    "         #  check if key 'description' has Non-None value in dictionary \n",
    "        if article['description']:\n",
    "            contents += article['description']+\" \"\n",
    "            \n",
    "    return contents\n",
    "\n",
    "contents = combine_articles(articles)\n",
    "print('Overall we have',len(contents),'characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTJt_NnohuW0"
   },
   "source": [
    "## Data cleaning, manipulation and visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrx-UF-ehuW1"
   },
   "source": [
    "Now begins the funny part\n",
    "1. Let's clean the data removing not interesting words\n",
    "2. Let's compute the frequency of use of words\n",
    "3. Let's visualize graphically the computed analysis - Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WHASt3fbhuW1",
    "outputId": "b68499b5-202f-4e15-a4cd-b580bf8f769e"
   },
   "outputs": [],
   "source": [
    "# 1 cleaning data\n",
    "def clean_data(content_to_clean):\n",
    "    print('Before cleaning')\n",
    "    print(contents[:600])\n",
    "    print()\n",
    "    # simple text normalization: string in lower case\n",
    "    content_to_clean = content_to_clean.lower()\n",
    "    # remove punctuations\n",
    "    cleaned_content = remove_punctuations(content_to_clean).split()\n",
    "    # remove stopwords\n",
    "    cleaned_content = remove_stopwords(cleaned_content)\n",
    "    print('After cleaning')\n",
    "    print(cleaned_content[:200])\n",
    "    print()\n",
    "    return cleaned_content\n",
    "\n",
    "cleaned_content = clean_data(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Km1bimgBhuW2"
   },
   "outputs": [],
   "source": [
    "# 2 computing word-frequency pairs\n",
    "def analyze_data(cleaned_content):\n",
    "    wordcount = {}\n",
    "    for w in cleaned_content:\n",
    "        if w in wordcount:\n",
    "            wordcount[w] += 1\n",
    "        else:\n",
    "            wordcount[w] = 1\n",
    "    return wordcount\n",
    "\n",
    "wordcount = analyze_data(cleaned_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "dig9Bd0ahuW2",
    "outputId": "c0579119-91d0-40ed-9768-a0d71a9509b5"
   },
   "outputs": [],
   "source": [
    "# 3 visualizing the analysisis in a Word Cloud\n",
    "def visualize_data(wordcount):\n",
    "    wc = WordCloud(width=1200,height=600, background_color='white').generate_from_frequencies(wordcount)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_data(wordcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZOihIUyhuW3"
   },
   "source": [
    "## Let's put everything in a function, so that we can run it easily for different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiLPRUQ-huW3"
   },
   "outputs": [],
   "source": [
    "def make_world_cloud(query,source,n_articles=50):\n",
    "    \"\"\"\n",
    "    Download relevant 'n_article' articles that contain 'query' from 'source' newspaper using NewsAPI\n",
    "    and plot a wordcloud with the most common words in the corpus.\n",
    "    :param query: \n",
    "        Keywords or phrases to search for in the article title and body.\n",
    "    :param source: \n",
    "        the news source or blog you want headlines from\n",
    "    :param n_articles:\n",
    "        The number of articles to return (maximum 100)\n",
    "    \"\"\"\n",
    "    # 1.  Download all the articles the online service NewsAPI\n",
    "    newsapi = NewsApiClient(api_key='345f8a0aa8c64d549fde1d8343d036f8')\n",
    "    json_data = newsapi.get_everything(q=query,           # All the articles that contain this word\n",
    "                                       language='en',\n",
    "                                       sources=source,      # The newspaper of interest\n",
    "                                       page_size=n_articles,  # Get 50 articles (by default)\n",
    "                                       sort_by='relevancy'\n",
    "                                    )\n",
    "    \n",
    "    # 2. Combine the articles into one document (string)\n",
    "    contents = combine_articles(json_data['articles'])\n",
    "    \n",
    "    # 3. cleaning data\n",
    "    cleaned_content = clean_data(contents)\n",
    "\n",
    "    # 4. computing word-frequency pairs\n",
    "    wordcount = analyze_data(cleaned_content)\n",
    " \n",
    "    # 5. Visualizing the analysis in a Word Cloud\n",
    "    visualize_data(wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "xHpV6Q20huW4",
    "outputId": "27ada0d5-37c5-4f00-9482-50d9a392b75a"
   },
   "outputs": [],
   "source": [
    "make_world_cloud('coronavirus','the-verge',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "r4lb3NzahuW5",
    "outputId": "57b4833e-acfa-4b66-89d7-ac2fd63956fc"
   },
   "outputs": [],
   "source": [
    "make_world_cloud('coronavirus','bbc-news',100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copia di 05WordCloudCovid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
