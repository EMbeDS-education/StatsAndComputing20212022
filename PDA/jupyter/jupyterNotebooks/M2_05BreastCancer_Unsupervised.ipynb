{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"+4\">Programming and Data Analytics 2021/2022</font></center>\n",
    "<center><font size=\"+2\">Module 2</font></center>\n",
    "<center><font size=\"+2\">Sant'Anna School of Advanced Studies, Pisa, Italy</font></center>\n",
    "<center><img src=\"https://github.com/EMbeDS-education/StatsAndComputing20202021/raw/main/IPDP/jupyter/jupyterNotebooks/images/SSSA.png\" width=\"700\" alt=\"The extensible parallel architecture of MultiVeStA\"></center>\n",
    "\n",
    "<center><font size=\"+2\">Course responsible</font></center>\n",
    "<center><font size=\"+2\">Andrea Vandin a.vandin@santannapisa.it</font></center>\n",
    "\n",
    "<center><font size=\"+2\">Co-lecturer </font></center>\n",
    "<center><font size=\"+2\">Daniele Licari d.licari@santannapisa.it</font></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"+2\">Part 2</font></center>\n",
    "<center><font size=\"+1\">Breast Cancer Diagnosis 2</font></center>\n",
    "<center><font size=\"+1\">Unsupervised learning</font></center>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook provides an an introduction to the unsupervised learning pipeline**\n",
    "   * Dimensionality Reduction (PCA)\n",
    "   * Clustering (K-means)\n",
    "\n",
    "You can find more details in the [APPENDIX](#APPENDIX) of this document.\n",
    "\n",
    "In particular, this notebook will introduce the libraries:\n",
    "\n",
    "   * [scikit-learn](https://scikit-learn.org/stable/): simple and efficient tools for predictive data analysis \n",
    "   * [Seaborn](http://seaborn.pydata.org/): seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References** \n",
    "\n",
    "Some in-depth study material:\n",
    "\n",
    "* <mark> [Statistics and Machine Learning in Python, E.Duchesnay, T.Löfstedt, F.Younes](https://duchesnay.github.io/pystatsml)</mark>\n",
    "* [Topics in Statistical Learning, Francesca Chiaromonte](https://github.com/EMbeDS-education/StatsAndComputing20202021/tree/main/TSL/slides)\n",
    "* [Python for Data Analysis, 2nd edition, William Wesley McKinney (O’Reilly)](https://www.oreilly.com/library/view/python-data-science/9781491912126/)\n",
    "* [Freely available Jupyter notebooks covering the examples/material of each chapter](https://github.com/jakevdp/PythonDataScienceHandbook/tree/master/notebooks)\n",
    "* [Introduction to Data Mining (2nd Edition), Pang-Ning Tan et al.](https://www.cse.msu.edu/~ptan/)\n",
    "* [Introduction to Machine Learning Algorithms, KNIME AG](https://www.knime.com/knime-course-material-download-page)\n",
    "\n",
    "Some pictures have been taken from these sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Benign and Malignant Breast Cancer Case Study \n",
    "We will analize Wisconsin Breast Cancer Dataset (WBCD), features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "<img src=\"images/Breast-Biopsy-2.jpg\" >\n",
    "\n",
    "![alt text](images/fna-benign1.png)\n",
    "![alt text](images/fna-malignant1.png)\n",
    "\n",
    "**Attribute Information**\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry \n",
    "- fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error, and \"worst\" or largest (mean of the three\n",
    "largest values) of these features were computed for each image,\n",
    "resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
    "13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "**Labels Class:**\n",
    "* malignant\n",
    "* benign\n",
    "\n",
    "\n",
    "\n",
    "This dataset is also available via the ftp server UW CS: http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/WDBC/\n",
    "\n",
    "![](images/machine_learning_cancer.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the all repository if you are on COLAB\n",
    "import os\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "course_name = 'StatsAndComputing20212022' \n",
    "\n",
    "if IN_COLAB: \n",
    "    !git clone https://github.com/EMbeDS-education/{course_name}.git\n",
    "    os.chdir(f'/content/{course_name}/PDA/jupyter/jupyterNotebooks') #for SSSA\n",
    "    # os.chdir(f'/content/{course_name}/jupyter/jupyterNotebooks')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing libs\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Visualizzation libs\n",
    "# keeps the plots in one place. calls image as static pngs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "from IPython.display import display, Markdown # display Markdown code using Python\n",
    "import seaborn as sns # data visualization library based on matplotlib\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Let's load preprocessed data by the Part 1 into Pandas, and get *Features matrix* and *Target array*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset, we will first load the data into a Pandas DataFrame object and display its content\n",
    "\n",
    "data = pd.read_csv('data/WBCD_preprocessed.csv', index_col=0)\n",
    "df_X = data.iloc[:,:-1] # Features matrix\n",
    "df_y = data.iloc[:,-1] # Target array\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "Datasets often have many (hundreds or even thousands) features and dropping redundant variables is not enough to reduce the dimensionality of the dataset.\n",
    "\n",
    "Too many variables can cause such problems below:\n",
    "* Increased computational cost\n",
    "* Too complex visualization problems\n",
    "* Decrease efficiency by including variables that have no effect on the analysis\n",
    "* Make data interpretation difficult\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "more details and other dimensionality reduction algorithms are listed in the [APPENDIX](#APPENDIX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML \n",
    "#  Dimensionality Reduction\n",
    "HTML('<iframe width=\"800\" height=\"550\" src=\"https://www.youtube.com/embed/smWJ-f8fSOY?mute=1\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more.\n",
    "\n",
    "![](images/pca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA reduces the dimensionality of a dataset, while preserving as much ‘variability’ (i.e.statistical information) as possible.\n",
    "It converts a set of possibly correlated predictors into a set of linearly uncorrelated variables**\n",
    "(using [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) of the data to project it to a lower dimensional space). \n",
    "\n",
    "<!-- PCA is based on two basic considerations: \n",
    "* high correlation between variables indicates redundancy in the data; \n",
    "* the most important variables express higher variance. \n",
    "\n",
    "Based on these considerations, the model simplifies the complexity of the variables; -->\n",
    "\n",
    "**PCA tries to explain the covariance structure of the data by means of a (hopefully small) number of components `(PC1, PC2,..,PCn)=PCA(X1,X2,...,Xn)`.These components are linear combinations of the original variables**, and often allow for an interpretation and a better understanding of the different sources of variation. \n",
    "\n",
    "<!-- ![](img/pca.png)  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "# Very Nive Principal component analysis (PCA)  youtube video by Serrano.Academy\n",
    "HTML('<iframe width=\"800\" height=\"550\" src=\"https://www.youtube.com/embed/g-Hb26agBFg?mute=1&cc_load_policy=1&fs=0&iv_load_policy=3&rel=1\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying PCA, we scale our data such that each feature has unit variance. This is necessary because fitting algorithms highly depend on the scaling of the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #for Scaling the features\n",
    "\n",
    "scaler = StandardScaler() # scaling data before PCA\n",
    "scaled_features =scaler.fit_transform(df_X.values)\n",
    "df_X_scaled = pd.DataFrame(scaled_features, index=df_X.index, columns=df_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*sklearn.decomposition.PCA*](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # Principal Component Analysis module\n",
    "\n",
    "pca2d = PCA(n_components=2)  \n",
    "pc = pca2d.fit_transform(df_X_scaled.values) # computes PCA components and trasforms original data\n",
    "\n",
    "pc_df = pd.DataFrame(data = pc , columns = ['PC1', 'PC2'])\n",
    "pc_df['target'] = df_y.values\n",
    "\n",
    "print(pc_df.shape)\n",
    "pc_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can create a visualization of our dataset\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=pc_df, x=\"PC1\", y=\"PC2\", hue=\"target\" # Grouping data points with different colors\n",
    ")\n",
    "\n",
    "plt.title('PCA Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  \n",
    "it can be deduced that the data are linearly separable and therefore a linear model could be an good solution to solve a classification problem. \n",
    "</div>\n",
    "\n",
    "\n",
    "*In the PCA, the next question is “how many principal components are we going to choose for our new feature subspace?*\n",
    "\n",
    "A number of principal components must be considered such that they **take into account a sufficiently high percentage of total variance** (at least 70%, for example, when stop increasing substantially). When defining the minimum percentage of acceptable variance, the number of original variables should be taken into account, so that as the number of variables increases, a lower percentage of explained variance may be accepted.\n",
    "\n",
    "**The explained variance tells us how much information (variance) can be attributed to each of the principal components**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2d.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca10 = PCA(n_components=10)\n",
    "pca10.fit(df_X_scaled.values)\n",
    "\n",
    "plt.figure(1, figsize=(14, 7))\n",
    "plt.bar(range(1,11,1), pca10.explained_variance_ratio_, alpha=0.5, align='center',\n",
    "        label='individual explained variance')\n",
    "plt.step(range(1,11,1),pca10.explained_variance_ratio_.cumsum(), where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.title(\"Around 95% of variance is explained by the Fisrt 10 components \");\n",
    "plt.legend(loc='best')\n",
    "plt.axhline(y=0.7, color='r', linestyle='-') # 70% of  explained variance\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PC1 describes most of the variability in the data, PC2 adds the next big contribution, and so on. In the end, the last PCs do not bring much more\n",
    "information to describe the data.\n",
    "* Thus, to describe the data we could use only the top m < n (i.e., i;, PC1, ⋯ PCn) components with little - if any - loss of information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>   A good number of principal component can be 6 because the total explained variance stops increasing substantially or I can choose 3 with the explained variance of 74% is enough (and the rest is noise)  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more dimensionality reduction techniques in the [APPENDIX](#APPENDIX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Discover hidden structures in unlabeled data (unsupervised).\n",
    "Clustering identifies a finite set of groups (clusters) C1, C2 ⋯ , Ck in the dataset such that:\n",
    "* Objects within the same cluster Ci shall be as similar as possible\n",
    "* Objects of different clusters Ci, Cj (i ≠ j) shall be as dissimilar as possible\n",
    "\n",
    "Clustering algorithms can be categorized based on their cluster model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "# Why Data Scaling is important in Machine Learning\n",
    "HTML('<iframe width=\"800\" height=\"500\" src=\"https://www.youtube.com/embed/cSrKc0A2gv0?end=60\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "**K-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean.**\n",
    "The means are commonly called the cluster “centroids”; \n",
    "![image](images/cluster1.jpg)\n",
    "**K-Means Algorithm:**\n",
    "1. Guess some cluster centers\n",
    "2. Repeat until converged\n",
    "    - *Find closest centroid*: assign points to the nearest cluster center\n",
    "    - *Update centroid*: set the cluster centers to the mean\n",
    "    \n",
    "\n",
    "\n",
    "<img src='images/1_rwYaxuY-jeiVXH0fyqC_oA.gif'>\n",
    "\n",
    "[sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html): The KMeans estimator class in scikit-learn perform  K-Means Clustering.\n",
    "\n",
    "<!-- 1. Getting  K values as input which is the number of clusters or centroids\n",
    "2. Selecting random centroids for each cluster\n",
    "3. Assigning each data point to its closest centroid (Euclidean distance)\n",
    "4. Adjusting the centroid for the newly formed cluster in step 3\n",
    "5. Repeating step 3 and 4 till all the data points are perfectly organised within a cluster space -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = df_X_scaled.values\n",
    "\n",
    " # k-means++ is an algorithm for choosing a good initial centroids (far away from each other). https://en.wikipedia.org/wiki/K-means%2B%2B\n",
    "kmeans = KMeans(n_clusters = 2, init = 'k-means++', random_state = 42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "kmeans.labels_"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Good Number Of Clusters\n",
    "Each cluster is formed by calculating and comparing the distances of data points within a cluster to its centroid. An ideal way to figure out the right number of clusters would be to calculate the Within-Cluster-Sum-of-Squares (WCSS or SSE). \n",
    "\n",
    "WCSS is the sum of squares of the distances of each data point in all clusters to their respective centroids.\n",
    "The total within-cluster sum of square measures the compactness (i.e goodness) of the clustering and we want it to be as small as possible.\n",
    "\n",
    "![image.png](images/1_zlZOSJB_DISgUxb06QwISw.png)\n",
    "\n",
    "We can find the good value for K using an Elbow point graph. We randomly initialise the K-Means algorithm for a range of K values and will plot it against the WCSS for each K value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "# Why Data Scaling is important in Machine Learning\n",
    "HTML('<iframe width=\"800\" height=\"500\" src=\"https://www.youtube.com/embed/cSrKc0A2gv0?start=62\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_X_scaled.values\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "n_clusters = 5\n",
    "\n",
    "for i in range(1, n_clusters+1):\n",
    "    #Compute kmeans for different values of k. For instance, by varying k from 1 to 5 clusters\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42) # k-means++ is an algorithm for choosing a good initial centroids. \n",
    "    # For each k, calculate the total within-cluster sum of square (wss)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1, n_clusters+1), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.axvline(x=2, color='r', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>   The “elbow” (the point of inflection on the curve) is a good indication that the underlying model fits best at that point.\n",
    "For the above-given graph, the good value for K would be 2 </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means++ is an algorithm for choosing the initial centroids. New centroid is a far point from the other centroids\n",
    "kmeans = KMeans(n_clusters = 2, init = 'k-means++', random_state = 42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "pc_df['cluster'] = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "# scatter plot to group data points with different colors \n",
    "sns.scatterplot(\n",
    "    data=pc_df, x=\"PC1\", y=\"PC2\", hue=\"cluster\" # Grouping data points with different colors\n",
    ")\n",
    "\n",
    "# to plot cluster centroids\n",
    "df_centroid_pca = pc_df.groupby('cluster').mean()\n",
    "plt.scatter(df_centroid_pca.iloc[:,0],df_centroid_pca.iloc[:,1], c='black', marker='x',s=100,\n",
    "            label=\"cluster centroids\")  \n",
    "\n",
    "# to plot cluster centroids\n",
    "df_mean_pca = pc_df.groupby('target').mean()\n",
    "plt.scatter(df_mean_pca.iloc[:,0],df_mean_pca.iloc[:,1], c='red',marker='+',s=50,\n",
    "            label=\"mean target class (b,m)\") \n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title('PCA Scatter Plot (blue cluster 0, orange cluster 1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  : It is interesting to see how the centroids of the clusters fall very close to the average values of the two classes of tumors (benign and malignant). Therefore it is possible to note that, if we did not have a labeled dataset (with well-defined classes B and M) we would still be able to determine (with good probability) the class of belonging of the dataset elements, through an unsupervised clustering process .</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is presented in the [APPENDIX](#APPENDIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appendix includes further information on the course topics but **will not be exam topics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dimensionality Reduction \n",
    "### Feature Selection using Scikit Learn\n",
    "Feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. \n",
    "* [SelectKBest()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) removes all but the k highest scoring features\n",
    "[The Chi-square (χ2) test](https://en.wikipedia.org/wiki/Chi-squared_test) is used to examine whether observed data fits with expected data.\n",
    "\n",
    "It is a test of independence and it is used to determine if there is a significant relationship between two variables. **Recall that the chi-square test measures dependence between stochastic variables**, so using this function “weeds out” the features that are the most likely to be independent of class and therefore irrelevant for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "def selector(X, y, k=12):\n",
    "    \n",
    "    \"\"\"The function receive features and labels (X, y) and a target number to select features (k)\n",
    "    base on the chi-square (χ2) and return a new dataframe wiht k best features\"\"\"\n",
    "    \n",
    "    selector =SelectKBest(chi2, k=k)\n",
    "    best_X = selector.fit_transform(X, y)\n",
    "    \n",
    "    return pd.DataFrame(best_X, columns=X.columns[selector.get_support()])\n",
    "\n",
    "best_X = selector(df_X, df_y, 5)\n",
    "\n",
    "best_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PCA\n",
    "PCA is a statistical procedure that (orthogonally) transforms the original n coordinates of a data set into a new set of n coordinates,\n",
    "called principal components.\n",
    "\n",
    "`(PC1, PC2,..,PCn)=PCA(X1,X2,...,Xn)`\n",
    "\n",
    "The first principal component PC1 follows the direction (eigenvector) of the largest possible variance (largest eigenvalue of the covariance matrix) in the data.\n",
    "\n",
    "Each succeeding component PCk follows the direction of the next largest possible variance under the constraint that it is orthogonal to (i.e., uncorrelated with) the preceding components\n",
    "\n",
    "In a nutshell, The principal components  are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. \n",
    "![](images/pca_2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import HTML\n",
    "# Extract from Principal component analysis (PCA)  youtube video by Serrano.Academy\n",
    "HTML('<iframe width=\"1200\" height=\"700\" src=\"https://www.youtube.com/embed/g-Hb26agBFg?start=1252&cc_load_policy=1&fs=0&iv_load_policy=3&rel=1\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D scatter plot\n",
    "pca3d = PCA(n_components=3)\n",
    "pc3 = pca3d.fit_transform(df_X_scaled.values)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D # 3D scatter plot\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "ax = Axes3D(fig) \n",
    "\n",
    "cmap = {'benign':'orange','malignant':'blue'}\n",
    "ax.scatter(pc3[:,0], pc3[:,1], pc3[:,2], c=[cmap[c] for c in  df_y.values],\n",
    "           marker='o', s=20)\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.view_init(30,-110)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Loadings\n",
    "It is also possible to visualize loadings using shapes, and use annotations to indicate which feature a certain loading original belong to. \n",
    "\n",
    "* PCA loading plot which shows how strongly each characteristic influences a principal component.\n",
    "\n",
    "For more details about the linear algebra behind eigenvectors and loadings, see this [Q&A thread](https://stats.stackexchange.com/questions/143905/loadings-vs-eigenvectors-in-pca-when-to-use-one-or-another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import biplot_pca\n",
    "biplot_pca( pc_df, pca2d, df_X_scaled.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing the exploratory process aimed at understanding how to simplify the dataset, without losing relevant information, the following processes were applied:  UMAP and TSNE , which are techniques for reducing complexity; in particular:\n",
    "### TSNE and UMAP\n",
    "* [TSNE (T-distributed Stochastic Neighbor Embedding)](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf) is a non-linear dimensionality reduction technique that is particularly suited to reducing the complexity of multidimensional datasets in a two- or three-dimensional model. **It is better than PCA, but it is computationally expensive**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "# see StatQuest: t-SNE, Clearly Explained video from youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/NEaUSP4YerM?mute=1&start=120&end=250&cc_load_policy=1&fs=0&iv_load_policy=3&rel=1\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [UMAP (Uniform Manifold Approximation and Projection)](https://arxiv.org/abs/1802.03426) is a dimension reduction technique that can be used for visualisation **similarly to t-SNE, but with superior run time performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "# see AICoffeeBreak UMAP explained | The best dimensionality reduction?\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6BPl81wGGP8?mute=1&cc_load_policy=1&fs=0&iv_load_policy=3&rel=1\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "By reducing the dimension in a way that preserves as much of the structure of the data as possible we can get a visualisable representation of the data allowing us to “see” the data and its structure and begin to get some intuition about the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  umap-learn for umap dimension reduction\n",
    "%pip install pip -U\n",
    "%pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "import time \n",
    "X = df_X_scaled.values\n",
    "y = df_y.values \n",
    "\n",
    "print(f\"[{time.asctime(time.localtime())}] Starting\")\n",
    "# Invoke the TSNE method\n",
    "tsne_results = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=2000).fit_transform(X)\n",
    "print(f\"[{time.asctime(time.localtime())}] Completed TSNE\")\n",
    "# Invoke the UMAP method\n",
    "reducer = UMAP().fit_transform(X) \n",
    "print(f\"[{time.asctime(time.localtime())}] Completed UMAP\")\n",
    "# Invoke the PCA method\n",
    "pc = PCA(n_components=2).fit_transform(X)\n",
    "print(f\"[{time.asctime(time.localtime())}] Completed PCA\")\n",
    "\n",
    "# Plot the TSNE and PCA visuals side-by-side\n",
    "cmap = {'benign':'orange','malignant':'blue'}\n",
    "fig = plt.figure(figsize = (15,9))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('TSNE Scatter Plot')\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1],  c =[cmap[x] for x in y] , alpha=0.75)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('UMAP Plot')\n",
    "plt.scatter( reducer[:,0], reducer[:,1], c =[cmap[x] for x in y] ,alpha=0.75)\n",
    "      \n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('PCA Plot')\n",
    "plt.scatter( pc[:,0], pc[:,1], c =[cmap[x] for x in y] ,alpha=0.75)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "[sklearn.cluster Module](https://scikit-learn.org/stable/modules/clustering.html) contains several Clustering models \n",
    "![image.png](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png)\n",
    "<!--  The next section introduces hierarchical clustering, but before we will introduce a well-known clustering quality measure. -->\n",
    "\n",
    "Clustering techniques can be divided into two approaches: partitional (like Kmeans) and hierarchical.\n",
    "We suggest [Amit Saxena et al., A review of clustering techniques and developments, 2017](https://doi.org/10.1016/j.neucom.2017.06.053) paper for comprehensive study on clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette: Clustering quality measure\n",
    "Silhouette-Coefficient measures the quality of clustering\n",
    "* a(x): distance of object x to its cluster representative\n",
    "* b(x): distance of object x to the representative of the second-best cluster\n",
    "* Silhouette s(x) of x\n",
    "![image.png](images/silhouette.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes silhouette coefficients for each point, and **average it out for all the samples to get the silhouette score**.\n",
    "\n",
    "**The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation).**\n",
    "The value of the silhouette ranges between [1, -1], where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "# The silhouette_score gives the average value for all the samples.\n",
    "silhouette_score(X,  kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import kmeeans_silhouette_analysis\n",
    "sns.set_style('darkgrid') \n",
    "kmeeans_silhouette_analysis(X, range(2, n_clusters+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hierarchical clustering is presented in the [APPENDIX](#APPENDIX). The aspects to look out for in Silhouette plots are cluster scores below the average silhouette score, wide fluctuations in the size of the clusters, and also the thickness of the silhouette plot.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\" style='color:black'>\n",
    "<b>Observations</b>  \n",
    "\n",
    "    \n",
    "* In our example, the analysis of the silhouette is used to choose an optimal value for the number of clusters. The silhouette plot shows that a n_clusters value of 5 and 6 is not good because they have the silhouette score lower than average scores, many negative values and also large fluctuations in the size of the silhouette plot. **From the analysis of the silhouette, a good number of k clusters appears to be 2 since it confirms what has already been expressed by the elbow method.**\n",
    "\n",
    "* Both Elbow method / SSE Plot and Silhouette method can be used interchangeably based on the details presented by the plots. It may be good idea to use both the plots just to make sure that you select most optimal number of clusters.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering\n",
    "\n",
    "In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. [wiki](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n",
    "**Construction of a hierarchy of clusters (dendrogram) by merging/separating clusters with minimum/maximum distance**\n",
    "* The agglomerative follows the bottom-up approach, which builds up clusters starting with single object and then merging these atomic clusters into larger and larger clusters using a linkage function, until all of the objects are finally lying in a single cluster or otherwise until certain termination conditions are satisfied**. \n",
    "* The divisive hierarchical clustering follows the top-down approach, which breaks up cluster containing all objects into smaller clusters, until each object forms a cluster on its own or until it satisfies certain termination conditions. The hierarchical methods usually lead to formation of dendrograms as shown.\n",
    "\n",
    "<img src='images/hclustering.jpg' />\n",
    "\n",
    "**Base Algorithm**\n",
    "1. Form initial clusters consisting of a single object, and compute the distance between each pair of clusters.\n",
    "2. Merge the two clusters having minimum distance.\n",
    "3. Calculate the distance between the new cluster and all other clusters.\n",
    "4. If there is only one cluster containing all objects: Stop, otherwise go to step 2. \n",
    "\n",
    "<img src='images/hclustering_measures.jpg' width='700' />\n",
    "\n",
    "We used the Ward linkage method, it has the highest performance in most situations, except when there were verylarge differences among cluster sizes. \n",
    "[A COMPARISON OF HIERARCHICAL METHODS FOR CLUSTERING FUNCTIONAL DATA](https://people.stat.sc.edu/Hitchcock/compare_hier_fda.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import plot_dendrograms\n",
    "plot_dendrograms(X) # Ward dendrom has well separated and compact clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage,median  # linkage analysis and dendrogram for visualization\n",
    "from scipy.cluster.hierarchy import fcluster  # simple clustering\n",
    "from scipy.spatial.distance import pdist, squareform # metric\n",
    "\n",
    "sns.set_style('whitegrid') \n",
    "X = df_X_scaled.values\n",
    "#Perform hierarchical/agglomerative clustering using Ward method. \n",
    "# Ward = Similarity of two clusters is based on the increase in squared error when two clusters are merged\n",
    "Z = linkage(X, method='ward', metric='euclidean') \n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "# plots dendograms\n",
    "dendrogram(\n",
    "    Z,    \n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=11.,\n",
    "    show_contracted=True,\n",
    "    distance_sort='descending',\n",
    "    truncate_mode = 'lastp',\n",
    "    p=50\n",
    ")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* horizontal lines are cluster merges\n",
    "* vertical lines tell you which clusters/labels were part of merge forming that new cluster\n",
    "* heights of the horizontal lines tell you about the distance that needed to be \"bridged\" to form the new cluster\n",
    "* a huge jump in distance is typically what we're interested to find the optimal number of clusters. \n",
    "\n",
    "The dendrogram function with Ward method divides the data into 2 groups (it cuts to 70% of the maximum length) by default \n",
    "https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html\n",
    "    \n",
    "The 2 clusters are well separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some automated Cut-Off selection methods [but they are not very reliable](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting clusters from dendogram\n",
    "k=2\n",
    "clusters = fcluster(Z, k, criterion='maxclust')\n",
    "# clusters\n",
    "pc_df['hcluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "# scatter plot to group data points with different colors \n",
    "sns.scatterplot(\n",
    "    data=pc_df, x=\"PC1\", y=\"PC2\", hue=\"hcluster\", palette=['blue','orange'] # Grouping data points with different colors\n",
    ")\n",
    "\n",
    "# to plot cluster centroids\n",
    "df_centroid_pca = pc_df.groupby('hcluster').mean()\n",
    "plt.scatter(df_centroid_pca.iloc[:,0],df_centroid_pca.iloc[:,1], c='black', marker='x',s=100,\n",
    "            label=\"cluster centroids\")  \n",
    "\n",
    "# to plot mean for Benign and Malignant \n",
    "df_mean_pca = pc_df.groupby('target').mean()\n",
    "plt.scatter(df_mean_pca.iloc[:,0],df_mean_pca.iloc[:,1], c='red',marker='+',s=50,\n",
    "            label=\"mean target class (b,m)\") \n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title('PCA Scatter Plot (blue cluster 1, orange cluster 2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between real centroids and hierarchical clustering centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(X,  clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
