---
title: "crossvalidation"
author: "J. Di Iorio, F. Chiaromonte"
date: "3/8/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\section{Libraries}
We are going to use \textbf{tidyverse} and \textbf{ggplot2}.

```{r}
library(tidyverse) # for data manipulation and visualization
library(ggplot2) # for plots
library(caret) # for statistical learning techniques
library(dslabs)
```


\section{Data}
We will use dataset already presented in the previous lessons (the \textbf{economics} dataset from \textbf{ggplot2} package) but also the \textbf{mnist_27} dataset in the \textbf{dslabs} package.

The \textbf{mnist} dataset is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. In the \textbf{mnist_27} data set we only include a randomly selected set of 2s and 7s along with the two predictors based on the proportion of dark pixels in the upper left and lower right quadrants respectively. The dataset is divided into training and test sets.

```{r}
data("mnist_27")
mnist_27$test%>% ggplot(aes(x_1, x_2, color = y)) +  geom_point()
```


\section{Crossvalidation}
We are going to perform CV by hand. Precisely we are going to perform:
\begin{enumerate}
\item Leave-one-out Crossvalidation (LOOCV)
\item $k$-folds Crossvalidation
\end{enumerate}

Before starting let us prepare the data.
```{r}
data(economics)
economics <- economics[dim(economics)[1]:(dim(economics)[1]-119),]
economics$index <- 1:120
plot(economics$uempmed)
```

\section{LOOCV}
In this approach, we reserve only one data point from the available dataset, and train the model on the rest of the data. This process iterates for each data point. 

```{r}
degree_list <- list()
span_values <- seq(0.1,1,0.1)
for(deg in 1:2){
  err <- list()
  for(k in 1:length(span_values)){
    score <- list()
    for(i in 1:(nrow(economics))){
      training = economics[-i,]
      model = loess(uempmed ~ index, data = training, span = span_values[k], degree=deg) 
      validation = economics[i,]
      pred = predict(model, validation)
      # error of ith fold
      score[[i]] = (validation$uempmed - pred)^2
    }
  err[[k]] <- mean(unlist(score),na.rm=TRUE) # returns a vector
  }
  degree_list[[deg]] <- err
}

# prepare dataframe for ggplot
spans <- rep(span_values,2)
degrees <- rep(c(1,2), each = length(span_values))
err <- unlist(degree_list)
df_toplot <- as.data.frame(cbind(spans,degrees,err))

# plot
p <- ggplot(df_toplot, aes(x=spans, y=err, group=factor(degrees))) + geom_point() + geom_line(aes(col=factor(degrees)))
p
```

Let us find the parameters corresponding to the minimum error.
```{r}
df_toplot[which(df_toplot$err==min(df_toplot$err)),]
```
Let us plot the resulting regression line.
```{r}
best <- df_toplot[which(df_toplot$err==min(df_toplot$err)),]
res <- loess(uempmed ~ index, data = economics, span =best$spans, degree=best$degrees) 
plot(economics$index, economics$uempmed)
lines(predict(res), col='blue')
```

\subsection{$k$-fold CV}
Let us validate the parameter using the $k$-fold Crossvalidation.
Below are the steps for it:
\begin{enumerate}
\item Randomly split your entire dataset into $k$ folds;
\item For each k-fold in your dataset, build your model on $k â€“ 1$ folds of the dataset. Then, test the model to check the effectiveness for $k$th fold;
\item Record the error you see on each of the predictions;
\item Repeat this until each of the $k$-folds has served as the test set;
\item The average of your $k$ recorded errors is called the cross-validation error and will serve as your performance metric for the model;
\end{enumerate}

Create the folds
```{r}
library(caret)
flds <- createFolds(1:120, k = 10, list = TRUE, returnTrain = FALSE)
```

```{r}
degree_list <- list()
span_values <- seq(0.1,1,0.1)
for(deg in 1:2){ #polinomials degree
  err <- list()
  for(k in 1:length(span_values)){ #smoothness
    score <- list()
    for(i in 1:10){
      validation <- economics[unlist(flds[i]),]
      training <- economics[unlist(flds[-i]),]
      model = loess(uempmed ~ index, data = training, span = span_values[k], degree=deg) 
      pred = predict(model, validation)
      score[[i]] <- mean((pred - validation$uempmed)^2, na.rm=TRUE)
    }
    err[[k]] <- mean(unlist(score))
  }
  degree_list[[deg]] <- unlist(err)
}


spans <- rep(span_values,2)
degrees <- rep(c(1,2), each = length(span_values))
err <- unlist(degree_list)
df_toplot <- as.data.frame(cbind(spans,degrees,err))


p <- ggplot(df_toplot, aes(x=spans, y=err, group=factor(degrees))) + geom_point() + geom_line(aes(col=factor(degrees)))
p
```

Let us find the parameters corresponding to the minimum error.
```{r}
df_toplot[which(df_toplot$err==min(df_toplot$err)),]
```

Let us plot the resulting regression line.
```{r}
best <- df_toplot[which(df_toplot$err==min(df_toplot$err)),]
res <- loess(uempmed ~ index, data = economics, span =best$spans, degree=best$degrees) 
plot(economics$index, economics$uempmed)
lines(predict(res), col='blue')
```

\section{Using caret for CV}
The crossvalidation is automatically implemented in the train function of the caret package. The caret train function lets us train different algorithms using similar syntax. So, for example, we can type:
```{r}
library(caret)
train_knn <- train(y ~ ., method = "knn", data = mnist_27$train)
y_hat_knn <- predict(train_knn, mnist_27$test, type = "raw")
confusionMatrix(y_hat_knn, mnist_27$test$y)
```
Including a tuning parameter, train automatically uses cross validation to decide among a few default values. You can quickly see the results of the crossvalidation using the \textbf{ggplot} function. The argument \textbf{highlight} highlights the max:

```{r}
ggplot(train_knn, highlight = TRUE)
```
By default, the cross validation is performed by taking 25 bootstrap samples comprised of 25\% of the observations. For the kNN method, the default is to try  
$k= (5,7,9)$. We change this using the \textbf{tuneGrid} parameter.
```{r}
set.seed(2008)
train_knn <- train(y ~ ., method = "knn", 
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(9, 71, 2)))
ggplot(train_knn, highlight = TRUE)
```
The best $k$ shown in the plot and the corresponding training set outcome distribution is accessible in the following way:
```{r}
train_knn$bestTune
train_knn$finalModel
```

The overall accuracy on the training set is:
```{r}
confusionMatrix(predict(train_knn, mnist_27$test, type = "raw"),mnist_27$test$y)
```

