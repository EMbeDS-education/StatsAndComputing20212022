---
title: "*Cross validation*"
author: "L. Insolia, F. Chiaromonte (special thanks to J. Di Iorio)"
date: "March 29th 2022"
output:
  pdf_document:
    toc: true
  html_document: default
urlcolor: blue
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) # just to clean the pdf files, not recommended in practice!
```


\section{Introduction}

\subsection{Libraries}

We are going to use:
\begin{itemize}
  \item \textbf{tidyverse}: \textit{Easily Install and Load the 'Tidyverse'}
  \item \textbf{caret}: \textit{Classification and Regression Training}
  \item \textbf{ggplot2}: \textit{Create Elegant Data Visualisations Using the Grammar of Graphics}
  \item \textbf{dslabs}: \textit{Data Science lab}
\end{itemize}

```{r}
library(tidyverse)  # data manipulation and visualization
library(caret)      # statistical learning techniques
library(ggplot2)    # plots
library(dslabs)     # digit recognition data
```


\subsection{Data}

We will use a dataset already presented in a previous lesson (the \textbf{economics} in \textbf{ggplot2} package) as well as the \verb|mnist_27| dataset which is part of the \textbf{dslabs} package.

The \textbf{mnist} dataset is a very famous and large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. In the \verb|mnist_27| data set, we only include a randomly selected set of 2's and 7's (the label/class that we try to predict) along with the two predictors based on the proportion of dark pixels in the upper left and lower right quadrants respectively. The dataset is divided into training and test sets.

```{r}
data("mnist_27")
mnist_27$test %>% ggplot(aes(x_1, x_2, color = y)) +  geom_point()
```


\section{Cross validation}

We are going to perform CV by hand. Precisely we are going to perform:
\begin{enumerate}
  \item Leave-one-out cross validation (LOOCV)
  \item $k$-folds cross validation
\end{enumerate}

Before starting let's prepare the data.
```{r}
data(economics)
economics <- economics[dim(economics)[1]:(dim(economics)[1]-119),]
economics$index <- 1:120
plot(economics$uempmed)
```

\subsection{LOOCV}
In this approach, we reserve only one data point from the available dataset, and train the model on the rest of the data. This process iterates for each data point. 

```{r}

set.seed(2022)

degree_list <- list()
span_values <- seq(0.05,1,length=10)
for(deg in 1:2){ #polynomial degree
  err <- list()
  for(k in 1:length(span_values)){ #smoothness
    score <- list()
    for(i in 1:(nrow(economics))){
      training = economics[-i,]
      model = loess(uempmed ~ index, data = training, span = span_values[k], degree=deg) 
      validation = economics[i,]
      pred = predict(model, validation)
      # error of ith fold
      score[[i]] = (validation$uempmed - pred)^2
    }
  # returns a vector with the average error for a given degree & span
  err[[k]] <- mean(unlist(score),na.rm=TRUE) 
  }
  degree_list[[deg]] <- err
}

# prepare dataframe for ggplot
spans <- rep(span_values,2)
degrees <- rep(c(1,2), each = length(span_values))
err <- unlist(degree_list)
df_toplot <- as.data.frame(cbind(spans,degrees,err))

# plot
p <- ggplot(df_toplot, aes(x=spans, y=err, group=factor(degrees))) + 
  geom_point() + geom_line(aes(col=factor(degrees)))
p
```

Let's find the parameters corresponding to the minimum error.
```{r}
best <- df_toplot[which(df_toplot$err==min(df_toplot$err)),]
best
```
Let's plot the resulting smoothed curve.
```{r}
res <- loess(uempmed ~ index, data = economics, span =best$spans, degree=best$degrees) 
plot(economics$index, economics$uempmed)
lines(predict(res), col='blue')
```

\subsection{$k$-fold CV}

Let's validate the parameter using the $k$-fold cross validation.

These are the steps we need to implement:
\begin{enumerate}
  \item Randomly split your entire dataset into $k$ folds;
  \item Iterate across each $k$th fold, which serves as a testing set, and train your model only on the remaining $kâ€“1$ folds;
  \item Test model accuracy/effectiveness on the $k$th fold, and record the ``error'' you see on each of the $k$ predictions;
  \item Repeat this until each of the $k$-folds has served as the test set;
  \item The average of your $k$ recorded errors is called the \textbf{cross-validation error} and will serve as a performance metric for the model.
\end{enumerate}

Create the folds:
```{r}
flds <- caret::createFolds(1:nrow(economics), 
                           k = 3, list = TRUE, 
                           returnTrain = FALSE)
flds
class(flds)
# yopu can use [[k]] or [k] to access the k-th element
flds[1]     # you need to "unlist" afterwards (see below)
flds[[1]]   # you do not

```

Perform an iteration similarly to the one for LOOCV:
```{r}
degree_list <- list()
for(deg in 1:2){ #polynomial degree
  err <- list()
  for(k in 1:length(span_values)){ #smoothness
    score <- list()
    for(i in 1:length(flds)){
      validation <- economics[unlist(flds[i]),]
      training <- economics[unlist(flds[-i]),]
      model = loess(uempmed ~ index, data = training, span = span_values[k], degree=deg) 
      pred = predict(model, validation)
      score[[i]] <- mean((pred - validation$uempmed)^2, na.rm=TRUE)
    }
    err[[k]] <- mean(unlist(score))
  }
  degree_list[[deg]] <- unlist(err)
}


spans <- rep(span_values,2)
degrees <- rep(c(1,2), each = length(span_values))
err <- unlist(degree_list)
df_toplot <- as.data.frame(cbind(spans,degrees,err))


p <- ggplot(df_toplot, aes(x=spans, y=err, group=factor(degrees))) + 
  geom_point() + geom_line(aes(col=factor(degrees)))
p
```

Let us find the parameters corresponding to the minimum error.
```{r}
df_toplot[which(df_toplot$err==min(df_toplot$err)),]
```

Let us plot the resulting regression line.
```{r}
best <- df_toplot[which(df_toplot$err==min(df_toplot$err)),]
res <- loess(uempmed ~ index, data = economics, span =best$spans, degree=best$degrees) 
plot(economics$index, economics$uempmed)
lines(predict(res), col='blue')
```

\section{Using caret for CV}

Cross validation is also implemented in the train function of the caret package. 
Here caret train function allows one to train different algorithms using the same syntax. 

So, for example, we can type:
```{r}
train_knn <- train(y ~ ., method = "knn", data = mnist_27$train)
y_hat_knn <- predict(train_knn, mnist_27$test, type = "raw")
confusionMatrix(y_hat_knn, mnist_27$test$y)
```


In the presence of a a tuning parameter, it automatically uses cross validation to decide among a few default values. You can quickly see the results of the cross validation using the \textbf{ggplot} function. The argument \textbf{highlight} highlights the max:

```{r}
ggplot(train_knn, highlight = TRUE)
```

By default, cross validation is performed by taking 25 bootstrap samples comprising 25\% of the observations. 
For the kNN method, the default is to try $k= (5,7,9)$. 

We can change this using the \textbf{tuneGrid} parameter.
```{r}
set.seed(2022)
train_knn <- train(y ~ ., method = "knn", 
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(9, 71, 2)))
ggplot(train_knn, highlight = TRUE)
```

The best $k$ shown in the plot and the corresponding training set outcome distribution is accessible as follows:
```{r}
train_knn$bestTune
train_knn$finalModel
```

The overall accuracy on the training set is:
```{r}
confusionMatrix(predict(train_knn, mnist_27$test, type = "raw"),mnist_27$test$y)
```