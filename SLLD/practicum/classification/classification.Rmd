---
title: "*Classification*"
author: "L. Insolia, F. Chiaromonte (special thanks to J. Di Iorio)"
date: "March 22nd 2022"
output:
  pdf_document:
    toc: true
  html_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) # just to clean the pdf files, not recommended in practice!
```


\section{Introduction}


\subsection{Libraries}
We are going to use:
\begin{itemize}
  \item \textbf{tidyverse}: \textit{Easily Install and Load the 'Tidyverse'}
  \item \textbf{caret}: \textit{Classification and Regression Training}
  \item \textbf{MASS}: \textit{Support Functions and Datasets for Venables and Ripley's MASS}
  \item \textbf{ggplot2}: \textit{Create Elegant Data Visualisations Using the Grammar of Graphics}
  \item \textbf{readxl}: \textit{Read Excel Files}
\end{itemize}

```{r}
library(tidyverse)  # for data manipulation and visualization
library(caret)      # for statistical learning techniques
library(MASS)       # for AIC based stepwise regression
library(ggplot2)    # for plots
library(klaR)       # for LDA and QDA partition
library(readxl)     # for reading xlsx files
```

\subsection{Data}

Today we are going to use the \textbf{Titanic} data set.  
It contains 1309 rows and 15 columns. Each row represents a passenger and the columns describe some of their attributes. 

```{r cars}
df <- read_excel("Titanic.xlsx")
head(df, 10)
```

Let's see the internal data structure.
```{r}
str(df)
```

The type of some variables is not the optimal one (e.g., gender as a numeric variable). 
We can change these through the \textbf{mutate} function in \textbf{dplyr}.
```{r}

df <- df %>%
  dplyr::mutate(across(c(pclass, survived,       # select features
                  Residence, body, Gender),       
                factor))                         # transform to factor

# this is the traditional approach through base R
# df$pclass     <- as.factor(df$pclass)
# df$survived   <- as.factor(df$survived)
# df$Residence  <- as.factor(df$Residence)
# df$body       <- as.factor(df$body)
# df$Gender     <- as.factor(df$Gender)
```


According to the column type, the function \textbf{summary} provides some summary statistics for each variable.
```{r}
summary(df)
```

Now, we drop the columns having more than 50 percent of NAs, as well as the ones that will not be useful for our analysis. 

We also drop all those rows without age information.

```{r}
# selecting columns
df <- df %>% 
  dplyr::select(-c(name,ticket, fare, cabin, 
                   embarked, boat, body, home.dest)) 

# filtering out rows
df <- df %>% filter(!is.na(age))

# our "final" dataset
dim(df)
summary(df)
```


\section{Logistic Regression}

To assess prediction accuracy, we split the data into training and testing sets (at random, with no replacement). These encompass 75 and 25 percent of the points, respectively.
```{r}
set.seed(123)
# set.seed(1)               # try to re-run the analysis with this
training_samples <- df$survived %>% 
  caret::createDataPartition(p = 0.75, list = FALSE)

train <- df[training_samples, ]
test  <- df[-training_samples, ]
```

\subsection{Simple Logistic Regression}

Using the training set, we build a simple logistic regression model using sex as the only explanatory variable of the survival status for each passenger.
```{r}
simple_glm <- glm(survived ~ Gender, 
                  data = train, family = 'binomial')
summary(simple_glm)
```

You can extract various inforation from the output object \verb|simple_glm|, such as the regression coefficients:
```{r}
simple_glm$coefficients
```

Our model is $log(\frac{p(x)}{1-p(x)}) = \beta_{0} + \beta_{1}Gender$, where $\beta_{0} = Intercept$ and $\beta_{1}=Gender1$. 
Therefore, $log(\frac{p(x)}{1-p(x)}) = Intercept +Gender1 \times Gender$.

Let's see predictive power of our model in terms of \textit{accuracy} -- i.e. the proportion of correct predictions, both true positives and true negatives, among the total number of cases examined.

Since our response (Survival) is a binary variable, we need to round the probabilities which are predicted by the logistic model.
```{r}

# Test for accuracy: predict test data
predict_sex_survived <- predict(simple_glm, newdata = test,type = 'response') 

# round up the predictions
predict_sex_survived <- ifelse(predict_sex_survived>0.5, 1, 0)

# calculate accuracy
accuracyRed <- mean(predict_sex_survived==test$survived)
accuracyRed
```

Our result is "pretty good" and it is also consistent with the "women and children first" code of conduct/policy (you can find more details on [Wikipedia](https://en.wikipedia.org/wiki/Women_and_children_first)).


\subsection{Multiple Logistic Regression -- Stepwise selection}

We may try to improve predictive power by including more features into the model. 
Let us start with the full/saturated model (i.e., the one comprising all features).
```{r}
glm_complete <- glm(survived ~ ., data=train, family = 'binomial')
summary(glm_complete)
```

Let's see its predictive accuracy.
Has it improved?
```{r}
# Test for accuracy: predict test data
predict_sex_survived <- predict(glm_complete, newdata = test,type = 'response') 

# round up the predictions
predict_sex_survived <- ifelse(predict_sex_survived>0.5, 1, 0)

# calculate accuracy
accuracySat <- mean(predict_sex_survived==test$survived)
accuracySat

confusionMatrix(as.factor(predict_sex_survived), test$survived)
```


Moreover, we notice that some features result as non-significant according to their $p$-values (last column of the previous table). 
We can select "relevant" predictors through stepwise regression, and then compute an information criterion such as AIC to compare different sub-models and pick the "best" one (e.g. minimizing the AIC). 

```{r}
glm_stepwise <- glm_complete %>% 
  MASS::stepAIC(direction='both', trace = T)
summary(glm_stepwise)
```

A comparison of the simple, saturated and the selected model using AIC indicate that the latter is a good compromise between the two. 
It should have a "comparable" predictive power but a lower complexity compared to the saturated one.
```{r}
AIC(simple_glm, glm_complete, glm_stepwise)
```

Using the selected model, we compute again the probability for survival on the test set, and the overall prediction accuracy.
```{r}

predict_sex_survived <- predict(glm_stepwise, newdata = test,type = 'response') 

predict_sex_survived <- ifelse(predict_sex_survived>0.5, 1, 0)

accuracy <- mean(predict_sex_survived==test$survived)
accuracy
```

We can create also a confusion matrix (and statistics) to compare our predictions for the test set.
```{r}
confusionMatrix(as.factor(predict_sex_survived), test$survived)
```

\section{LDA and QDA}

Let us apply LDA and QDA to a multi label dataset such as \textbf{iris}. We are going to use just the first two columns with a gaussian noise.
```{r}
iris2 <- iris[,c(1,2,5)]
species_name <- iris$Species
iris2[,1] <- iris2[,1] + rnorm(150, sd=0.025)
iris2[,2] <- iris2[,2] + rnorm(150, sd=0.025)

plot(iris2[,1:2], main='Iris.Sepal', xlab='Sepal.Length', ylab='Sepal.Width', pch=15)
points(iris2[1:50,], col=2, pch=15)
points(iris2[51:100,], col=4, pch=15)
points(iris2[101:150,], col=3, pch=15)
legend(min(iris[,1]), max(iris[,2]), legend=levels(species_name), fill=c(2,3,4))

```
Once again we create a train set and a test set.
```{r}
set.seed(123)
training.samples <- species_name %>%
  createDataPartition(p = 0.8, list = FALSE)
train <- iris2[training.samples, ]
test <- iris2[-training.samples, ]
```

It is generally recommended to standardize/normalize continuous predictor before the analysis.
```{r}

help(preProcess)

# Estimate preprocessing parameters
preproc.param <- train %>% 
  preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
train_transformed <- preproc.param %>% predict(train)
test_transformed <- preproc.param %>% predict(test)


```

\subsection{LDA}

Before performing LDA, consider:
\begin{itemize}
  \item Inspecting the univariate distributions of each variable and check whether they are normally distributed. If not, you can transform them using log and root for exponential distributions and Box-Cox for skewed distributions.
  \item Standardize the variables to make their scale comparable.
  \item Be careful for the possible presence of outliers, and remember: ``\textit{any reasonable, formal or informal, procedure for rejecting outliers will prevent the worst}'' (P.J. Huber).
\end{itemize}
```{r}

help(lda)

lda.iris <- lda(factor(Species)~ Sepal.Length + Sepal.Width, data=train_transformed)
lda.iris
```

In the output above, aside from prior provability and group means, we have:
\begin{itemize}
  \item \textit{Coefficients of linear discriminants}: the linear combination of predictor variables that are used to form the LDA decision rule.
  \item \textit{Proportion of trace}: the separation achieved by each discriminant function (in percentage).
\end{itemize}

Here is the model accuracy on training data.
```{r}
predmodel.train.lda = predict(lda.iris, data=train_transformed)
confusionMatrix(as.factor(predmodel.train.lda$class), train_transformed$Species)

```


The plot below shows how the response class has been classified by the LDA classifier. 
The $x$-axis shows the value of the line defined by the coefficient of linear discriminant for LDA. 
Groups are the ones in the response classes.
```{r}
# first discriminant
ldahist(predmodel.train.lda$x[,1], g= predmodel.train.lda$class) 
# second discriminant
ldahist(predmodel.train.lda$x[,2], g= predmodel.train.lda$class) 
```

See new $x$ with original labels
```{r}
#convert to data frame 
newdata <- data.frame(type = train_transformed$Species, lda = predmodel.train.lda$x)
library(ggplot2)
ggplot(newdata) + geom_point(aes(lda.LD1, lda.LD2, colour = type), size = 2.5)
```

See geometric division
```{r}
# library(klaR)
partimat(factor(Species)~ Sepal.Length + Sepal.Width, 
         data=train_transformed, method = "lda")
```

Now we check the model accuracy on test data.
```{r}
predmodel.test.lda = predict(lda.iris, newdata=test_transformed)
confusionMatrix(as.factor(predmodel.test.lda$class), test_transformed$Species)
```


Let's try LDA with all variables in iris data:
```{r}

irisLda <- lda(Species~.,data=iris)
irisLda

scalIris <- scale(as.matrix(iris[,-5]),scale=FALSE)

irisProjection <- cbind(scalIris %*% irisLda$scaling, iris[,5,drop=FALSE])  

p <- ggplot(data=irisProjection,aes(x=LD1,y=LD2,col=Species))
p + geom_point()   

```


Geometric division
```{r}

partimat(factor(Species)~., 
         data=iris, method = "lda")

```


\subsection{QDA}

Next we will fit the model trough QDA 
The command is similar to LDA and it outputs the prior probabilities and Group means. 
Note that "Prior Probabilities" and "Group Means" values are same as of LDA.

```{r}
qda.iris <- qda(factor(Species)~ Sepal.Length + Sepal.Width, data=train_transformed)
qda.iris
```

We will find the model accuracy for training data.
```{r}
predmodel.train.qda = predict(qda.iris, data=train_transformed)
confusionMatrix(as.factor(predmodel.train.qda$class), train_transformed$Species)
```

We can see the geometric partition
```{r}
# library(klaR)
partimat(factor(Species) ~ Sepal.Length + Sepal.Width, 
         data=train_transformed, method = "qda", 
         col.correct='green', col.wrong='red')
```
 
 
 Let's also try QDA on all original variables
```{r}
irisQda <- lda(Species~.,data=iris)
irisQda

partimat(Species ~ ., data = iris, method = "qda", 
         plot.matrix = TRUE, col.correct='green', col.wrong='red')
```
 
 
\section{$k$NN}

We are going to use the \textbf{knn3()} function within \textbf{caret} package on the iris2 data (i.e. working on 2 dimensions).
Let's train the knn with $k = 1,2,3,10$
```{r}
knn_iris1  <- knn3(factor(Species) ~ Sepal.Length + Sepal.Width, 
                   data=train_transformed, k = 1)
knn_iris2  <- knn3(factor(Species) ~ Sepal.Length + Sepal.Width, 
                   data=train_transformed, k = 2)
knn_iris3  <- knn3(factor(Species) ~ Sepal.Length + Sepal.Width, 
                   data=train_transformed, k = 3)
knn_iris10 <- knn3(factor(Species) ~ Sepal.Length + Sepal.Width, 
                   data=train_transformed, k = 10)
```

Hand-made KNN:
```{r}
plot(test_transformed[,1:2], main='Iris.Sepal', 
     xlab='Sepal.Length', ylab='Sepal.Width',pch=16, col='grey')
points(train_transformed[which(train_transformed$Species=="setosa"),1:2], 
       col='red', pch=1)
points(train_transformed[which(train_transformed$Species=="virginica"),1:2], 
       col='blue', pch=1)
points(train_transformed[which(train_transformed$Species=="versicolor"),1:2], 
       col='green', pch=1)
```

And now let us predict new points labels in the test set.
Using $k=1$:
```{r}
predict(knn_iris1, test_transformed, type='prob')
predict_test_knn1 <- predict(knn_iris1, test_transformed, type='class')
confusionMatrix(predict_test_knn1, test_transformed$Species)
```

Using $k=2$:
```{r}
predict(knn_iris2, test_transformed, type='prob')
predict_test_knn2 <- predict(knn_iris2, test_transformed, type='class')
confusionMatrix(predict_test_knn2, test_transformed$Species)
```

Using $k=3$:
```{r}
# k=3
predict(knn_iris3, test_transformed, type='prob')
predict_test_knn3 <- predict(knn_iris3, test_transformed, type='class')
confusionMatrix(predict_test_knn3, test_transformed$Species)
```

Using $k=10$
```{r}
# k=10
predict(knn_iris10, test_transformed, type='prob')
predict_test_knn10 <- predict(knn_iris10, test_transformed, type='class')
confusionMatrix(predict_test_knn10, test_transformed$Species)
```
