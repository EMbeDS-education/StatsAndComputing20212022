---
title: "*Best Subset Selection*"
author: "L. Insolia, F. Chiaromonte (special thanks to J. Di Iorio)"
date: "May 11th 2022"
output:
  pdf_document:
    toc: true
  html_document: default
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) # just to clean the pdf files, not recommended in practice!
```


\section{Introduction}

\subsection{Libraries}

We are going to use:
\begin{itemize}
  \item \textbf{caret}: \textit{Classification and Regression Training}
  \item \textbf{leaps}: \textit{Regression Subset Selection}
\end{itemize}

```{r}
library(caret)  # statistical learning techniques
library(leaps)  # BSS
```

\subsection{Data}


We will use the \textbf{Body Fat dataset} (which is available in the [Datasets folder](https://github.com/EMbeDS-education/StatsAndComputing20212022/wiki/Datasets) of our course). 
See also the practicum material from yesterday.

The data concerns a sample of 252 men, and contains 15 different variables. We want to understand if we can reliably describe and predict body fat percentage on the basis of these variables, using regression. For age, we only have a binary indicator separating men below and above 45 years. The body measurements, on the other hand, are all continuous variables. 
```{r}
df <- read.table('BODY_FAT.TXT', header=TRUE)
names(df)
```

We want to predict "SiriBF." using all other features aside from "Density".
So we drop the "Density" column.
```{r}
df <- df[,-1]
```


\section{Best Subset Selection (Linear Regression)}


We will use the \textbf{regsubsets()} function (part of the \textbf{leaps} library). 
It performs best subset selection by identifying the best model that contains a given number of predictors, where the notion of "best" is based on the in-sample RSS. 
No cross-validation is performed. 
The summary() command outputs the best set of variables for each model size.
```{r}
regfit.full = regsubsets(SiriBF. ~ ., data = df,  nvmax = 13, method="exhaustive")
summary(regfit.full)
```

The summary() function also returns $R^2$, $RSS$ $R^{2}_{adj}$, $C_{p}$, and $BIC$. 
We can examine these to try to select the best overall model.
```{r}
names(summary(regfit.full))
```

As expected, the $R^2$ statistic increases monotonically as more variables are included into the model.
```{r}
summary(regfit.full)$rsq
#plot rss
plot(summary(regfit.full)$rsq, type="b")
```

Let us plot also the other indexes.
```{r}
reg.summary <- summary(regfit.full)
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",type="l")


plot(reg.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
max_adjr2 <- which.max(reg.summary$adjr2)
abline(v=max_adjr2, col="red", lty=2)
points(max_adjr2,reg.summary$adjr2[max_adjr2], col="red",cex=2,pch=20)

plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type='l')
min_cp <- which.min(reg.summary$cp )
points(min_cp, reg.summary$cp[min_cp],col="red",cex=2,pch=20)
abline(v=min_cp, col="red", lty=2)

plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",type='l')
min_bic <- which.min(reg.summary$bic)
points(min_bic,reg.summary$bic[min_bic],col="red",cex=2,pch=20)
abline(v=min_bic, col="red", lty=2)
```

Note that the use of different criteria leads to different decisions.

Here is another useful visualization tool which is available in leaps:
```{r}
plot(regfit.full,scale="bic") #for "bic"
plot(regfit.full,scale="adjr2") #for "adjr2"
```


\subsection{Tuning strategies}

To select the best model we are going to use cross-validation.

For instance, we can start by splitting the observations into a training set and a test set (of size 80 vs 20%).
```{r}

dim(df)

set.seed(1)
train=sample(1:nrow(df), round(nrow(df)*0.8), rep=F)
test = which(!(1:nrow(df) %in% train))

length(test)
length(train)
```

Now, we apply regsubsets() on the training set:
```{r}
regfit.best <- regsubsets(SiriBF. ~ ., data = df[train,],  nvmax = 13,
                          method="exhaustive")
```

We then compute the test set error for the best model of each size.
We need to compute all of this by hand, since the leaps library does not provide a predict function (as it is customary for R packages).
Thus, we first make a model matrix containing test data:
```{r}
test.mat = model.matrix(SiriBF. ~ ., data = df[test,])
dim(test.mat)
```

Within a for loop, for each size $i$, we extract the coefficients for the best model of that size. We generate the predictions by multiplying them into the appropriate columns of the test model matrix. Then, we compute the MSPE on test data.
```{r}
p = (ncol(df)-1) # number of predictors
mse <- rep(NA, p)
for(i in 1:p){
  coefi=coef(regfit.best,id=i)
  pred=test.mat[,names(coefi)]%*%coefi
  mse[i]=mean((df$SiriBF.[test]-pred)^2)
}

plot(mse, type='b')
abline(v=which.min(mse), col="red", lty=2)
legend(x = "bottomright",
       legend = "min test-MSPE",
       lty = 2,
       col = "red")
```

The best model is the one with 5 parameters (plus the intercept term). 
Let us see the coefficients:
```{r}
coef(regfit.best, which.min(mse))
```

We now perform best subset selection on the full data set and select the best 5 variables. 
```{r}
regfit.best.full <- regsubsets(SiriBF. ~ ., data=df ,nvmax=13)
coef(regfit.best.full, which.min(mse))
```

Focusing on the full dataset, we do not necessarily retrieve the same 5 variables that were obtained from the training set alone.
However, in this example we retrieve the same set of selected features.
```{r}
rescompare <- rbind(coef(regfit.best, which.min(mse)), 
                    coef(regfit.best.full, which.min(mse)))
rownames(rescompare) <- c("train", "full")
rescompare
```

We now consider a k-fold CV (with 10-folds).
Let's create 10 folds through the caret package:
```{r}
k = 10
set.seed(123)

# folds
folds <- createFolds(1:nrow(df), k = 10, list = TRUE, returnTrain = T)
fold <- matrix(NA, nrow(df), k)
for (i in 1:k) {
  fold[, i] <- (1:nrow(df) %in% folds[[i]])
}
head(fold, 10)


# initialize an empty matrix to contain test errors
cv.errors=matrix(NA, k, # num of folds
                 p,     # num of variables
                 dimnames=list(NULL, paste(1:p)))
```

Now we write a for loop that performs cross-validation.  We make our predictions for each model size, compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix cv.errors.
Considering the absence of a predict() function we create our own function where objects would be the result of regsubset(), newdata comprising the test set, and id as the number of parameters in the models obtained by regsubset().
```{r}
predict.regsubsets =function (object, newdata, id, ...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
```

We are ready!
```{r}
# loop for each fold
for(j in 1:k){
  best.fit = regsubsets(SiriBF. ~ . , data=df[fold[,j], ], nvmax = p)
  
  # for each best model
  for (i in 1:p){
    pred = predict.regsubsets(best.fit, df[!fold[,j], ], id = i)
    cv.errors[j, i] = mean((df$SiriBF.[!fold[,j]] - pred)^2)
  }
}
```

The result is stored in the cv.errors matrix having on the rows the folds and on the columns the number of variables of the model. Each cell contains the MSPE.
Now we compute column-wise averages.
```{r}
mean_mse <- colMeans(cv.errors)
plot(mean_mse, type='b')
abline(v=which.min(mean_mse), col="red", lty=2)
legend(x = "bottomright",
       legend = "min CV-MSPE",
       lty = 2,
       col = "red")
```

Using this approach we should retain 4 variables (plus the intercept):
```{r}
reg.best  <- regsubsets (SiriBF. ~ ., data=df, nvmax=p)
coef(reg.best, which.min(mean_mse))
```

\subsection{Scalable BSS}

Have a look at the \textbf{best-subset} package if you need to perform best subset selection on a large number of features.

```{r}
# library(devtools)
# install_github(repo="ryantibs/best-subset", subdir="bestsubset")
```


\subsection{Forward and Backward Stepwise Selection}

The regsubsets() function can be used to perform forward stepwise or backward stepwise selection. In order to do so, we need  to set the argument method=“forward” or method=“backward” (as opposed to "exhaustive").
```{r}
regfit.fwd = regsubsets(SiriBF. ~. , data=df,nvmax=13, method ="forward")
regfit.bwd = regsubsets(SiriBF. ~. , data=df,nvmax=13, method ="backward")
summary(regfit.fwd)
summary(regfit.bwd)
```

We can notice that the best k-variables models can be different according to the stepwise procedure.


\section{GLMs}

In the case of GLM (see, for instance, logistic regression), we can use the \textbf{bestglm} library. 
It performs best subset selection for GLMs and computes AIC, BIC, EBIC, BICq or Cross-Validation. 
It also calls the leaps library when performing linear regression.

See also the \textbf{glmulti} library.

